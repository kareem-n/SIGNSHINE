{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6116155,"sourceType":"datasetVersion","datasetId":2852448}],"dockerImageVersionId":30558,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Add Python module called Mediapipe\n!pip install mediapipe","metadata":{"execution":{"iopub.status.busy":"2023-09-14T06:23:34.011360Z","iopub.execute_input":"2023-09-14T06:23:34.011722Z","iopub.status.idle":"2023-09-14T06:23:52.617919Z","shell.execute_reply.started":"2023-09-14T06:23:34.011693Z","shell.execute_reply":"2023-09-14T06:23:52.616237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import the much needed stuff for training\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport mediapipe as mp\nimport os\nimport csv\nimport cv2 as cv\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom sklearn.metrics import confusion_matrix\n\nfrom keras.utils import to_categorical\nimport seaborn as sns\n# Make numpy values easier to read.\nnp.set_printoptions(precision=3, suppress=True)\n\n# Checking Tensorflow Version\ntf.__version__","metadata":{"execution":{"iopub.status.busy":"2023-09-14T06:23:52.620255Z","iopub.execute_input":"2023-09-14T06:23:52.620608Z","iopub.status.idle":"2023-09-14T06:24:04.545382Z","shell.execute_reply.started":"2023-09-14T06:23:52.620576Z","shell.execute_reply":"2023-09-14T06:24:04.544248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to Extract Feature from images or Frame\ndef extract_feature(input_image):\n    mp_hands = mp.solutions.hands\n    mp_drawing = mp.solutions.drawing_utils \n    image = cv.imread(input_image)\n    with mp_hands.Hands(static_image_mode=True, max_num_hands=2, min_detection_confidence=0.1) as hands:\n        while True:\n            results = hands.process(cv.flip(cv.cvtColor(image, cv.COLOR_BGR2RGB), 1))\n            image_height, image_width, _ = image.shape\n            # Print handedness (left v.s. right hand).\n            # Caution : Uncomment these print command will resulting long log of mediapipe log\n            #print(f'Handedness of {input_image}:')\n            #print(results.multi_handedness)\n\n            # Draw hand landmarks of each hand.\n            # Caution : Uncomment these print command will resulting long log of mediapipe log\n            #print(f'Hand landmarks of {input_image}:')\n            if not results.multi_hand_landmarks:\n                # Here we will set whole landmarks into zero as no handpose detected\n                # in a picture wanted to extract.\n                \n                # Wrist Hand\n                wristX = 0\n                wristY = 0\n                wristZ = 0\n                \n                # Thumb Finger\n                thumb_CmcX = 0\n                thumb_CmcY = 0\n                thumb_CmcZ = 0\n                \n                thumb_McpX = 0\n                thumb_McpY = 0\n                thumb_McpZ = 0\n                \n                thumb_IpX = 0\n                thumb_IpY = 0\n                thumb_IpZ = 0\n                \n                thumb_TipX = 0\n                thumb_TipY = 0\n                thumb_TipZ = 0\n\n                # Index Finger\n                index_McpX = 0\n                index_McpY = 0\n                index_McpZ = 0\n                \n                index_PipX = 0\n                index_PipY = 0\n                index_PipZ = 0\n                \n                index_DipX = 0\n                index_DipY = 0\n                index_DipZ = 0\n                \n                index_TipX = 0\n                index_TipY = 0\n                index_TipZ = 0\n\n                # Middle Finger\n                middle_McpX = 0\n                middle_McpY = 0\n                middle_McpZ = 0\n                \n                middle_PipX = 0\n                middle_PipY = 0\n                middle_PipZ = 0\n                \n                middle_DipX = 0\n                middle_DipY = 0\n                middle_DipZ = 0\n                \n                middle_TipX = 0\n                middle_TipY = 0\n                middle_TipZ = 0\n\n                # Ring Finger\n                ring_McpX = 0\n                ring_McpY = 0\n                ring_McpZ = 0\n                \n                ring_PipX = 0\n                ring_PipY = 0\n                ring_PipZ = 0\n                \n                ring_DipX = 0\n                ring_DipY = 0\n                ring_DipZ = 0\n                \n                ring_TipX = 0\n                ring_TipY = 0\n                ring_TipZ = 0\n\n                # Pinky Finger\n                pinky_McpX = 0\n                pinky_McpY = 0\n                pinky_McpZ = 0\n                \n                pinky_PipX = 0\n                pinky_PipY = 0\n                pinky_PipZ = 0\n                \n                pinky_DipX = 0\n                pinky_DipY = 0\n                pinky_DipZ = 0\n                \n                pinky_TipX = 0\n                pinky_TipY = 0\n                pinky_TipZ = 0\n                \n                # Set image to Zero\n                annotated_image = 0\n\n                # Return Whole Landmark and Image\n                return (wristX, wristY, wristZ,\n                        thumb_CmcX, thumb_CmcY, thumb_CmcZ,\n                        thumb_McpX, thumb_McpY, thumb_McpZ,\n                        thumb_IpX, thumb_IpY, thumb_IpZ,\n                        thumb_TipX, thumb_TipY, thumb_TipZ,\n                        index_McpX, index_McpY, index_McpZ,\n                        index_PipX, index_PipY, index_PipZ,\n                        index_DipX, index_DipY, index_DipZ,\n                        index_TipX, index_TipY, index_TipZ,\n                        middle_McpX, middle_McpY, middle_McpZ,\n                        middle_PipX, middle_PipY, middle_PipZ,\n                        middle_DipX, middle_DipY, middle_DipZ,\n                        middle_TipX, middle_TipY, middle_TipZ,\n                        ring_McpX, ring_McpY, ring_McpZ,\n                        ring_PipX, ring_PipY, ring_PipZ,\n                        ring_DipX, ring_DipY, ring_DipZ,\n                        ring_TipX, ring_TipY, ring_TipZ,\n                        pinky_McpX, pinky_McpY, pinky_McpZ,\n                        pinky_PipX, pinky_PipY, pinky_PipZ,\n                        pinky_DipX, pinky_DipY, pinky_DipZ,\n                        pinky_TipX, pinky_TipY, pinky_TipZ,\n                        annotated_image)\n            \n            annotated_image = cv.flip(image.copy(), 1)\n            for hand_landmarks in results.multi_hand_landmarks:\n                # Wrist Hand /  Pergelangan Tangan\n                wristX = hand_landmarks.landmark[mp_hands.HandLandmark.WRIST].x * image_width\n                wristY = hand_landmarks.landmark[mp_hands.HandLandmark.WRIST].y * image_height\n                wristZ = hand_landmarks.landmark[mp_hands.HandLandmark.WRIST].z\n\n                # Thumb Finger / Ibu Jari\n                thumb_CmcX = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_CMC].x * image_width\n                thumb_CmcY = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_CMC].y * image_height\n                thumb_CmcZ = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_CMC].z\n                \n                thumb_McpX = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_MCP].x * image_width\n                thumb_McpY = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_MCP].y * image_height\n                thumb_McpZ = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_MCP].z\n                \n                thumb_IpX = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_IP].x * image_width\n                thumb_IpY = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_IP].y * image_height\n                thumb_IpZ = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_IP].z\n                \n                thumb_TipX = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP].x * image_width\n                thumb_TipY = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP].y * image_height\n                thumb_TipZ = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP].z\n\n                # Index Finger / Jari Telunjuk\n                index_McpX = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_MCP].x * image_width\n                index_McpY = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_MCP].y * image_height\n                index_McpZ = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_MCP].z\n                \n                index_PipX = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_PIP].x * image_width\n                index_PipY = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_PIP].y * image_height\n                index_PipZ = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_PIP].z\n                \n                index_DipX = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_DIP].x * image_width\n                index_DipY = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_DIP].y * image_height\n                index_DipZ = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_DIP].z\n                \n                index_TipX = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].x * image_width\n                index_TipY = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].y * image_height\n                index_TipZ = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].z\n\n                # Middle Finger / Jari Tengah\n                middle_McpX = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_MCP].x * image_width\n                middle_McpY = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_MCP].y * image_height\n                middle_McpZ = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_MCP].z\n                \n                middle_PipX = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_PIP].x * image_width\n                middle_PipY = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_PIP].y * image_height\n                middle_PipZ = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_PIP].z\n                \n                middle_DipX = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_DIP].x * image_width\n                middle_DipY = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_DIP].y * image_height\n                middle_DipZ = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_DIP].z\n                \n                middle_TipX = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_TIP].x * image_width\n                middle_TipY = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_TIP].y * image_height\n                middle_TipZ = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_TIP].z\n\n                # Ring Finger / Jari Cincin\n                ring_McpX = hand_landmarks.landmark[mp_hands.HandLandmark.RING_FINGER_MCP].x * image_width\n                ring_McpY = hand_landmarks.landmark[mp_hands.HandLandmark.RING_FINGER_MCP].y * image_height\n                ring_McpZ = hand_landmarks.landmark[mp_hands.HandLandmark.RING_FINGER_MCP].z\n                \n                ring_PipX = hand_landmarks.landmark[mp_hands.HandLandmark.RING_FINGER_PIP].x * image_width\n                ring_PipY = hand_landmarks.landmark[mp_hands.HandLandmark.RING_FINGER_PIP].y * image_height\n                ring_PipZ = hand_landmarks.landmark[mp_hands.HandLandmark.RING_FINGER_PIP].z\n                \n                ring_DipX = hand_landmarks.landmark[mp_hands.HandLandmark.RING_FINGER_DIP].x * image_width\n                ring_DipY = hand_landmarks.landmark[mp_hands.HandLandmark.RING_FINGER_DIP].y * image_height\n                ring_DipZ = hand_landmarks.landmark[mp_hands.HandLandmark.RING_FINGER_DIP].z\n                \n                ring_TipX = hand_landmarks.landmark[mp_hands.HandLandmark.RING_FINGER_TIP].x * image_width\n                ring_TipY = hand_landmarks.landmark[mp_hands.HandLandmark.RING_FINGER_TIP].y * image_height\n                ring_TipZ = hand_landmarks.landmark[mp_hands.HandLandmark.RING_FINGER_TIP].z\n\n                # Pinky Finger / Jari Kelingking\n                pinky_McpX = hand_landmarks.landmark[mp_hands.HandLandmark.PINKY_MCP].x * image_width\n                pinky_McpY = hand_landmarks.landmark[mp_hands.HandLandmark.PINKY_MCP].y * image_height\n                pinky_McpZ = hand_landmarks.landmark[mp_hands.HandLandmark.PINKY_MCP].z\n                \n                pinky_PipX = hand_landmarks.landmark[mp_hands.HandLandmark.PINKY_PIP].x * image_width\n                pinky_PipY = hand_landmarks.landmark[mp_hands.HandLandmark.PINKY_PIP].y * image_height\n                pinky_PipZ = hand_landmarks.landmark[mp_hands.HandLandmark.PINKY_PIP].z\n                \n                pinky_DipX = hand_landmarks.landmark[mp_hands.HandLandmark.PINKY_DIP].x * image_width\n                pinky_DipY = hand_landmarks.landmark[mp_hands.HandLandmark.PINKY_DIP].y * image_height\n                pinky_DipZ = hand_landmarks.landmark[mp_hands.HandLandmark.PINKY_DIP].z\n                \n                pinky_TipX = hand_landmarks.landmark[mp_hands.HandLandmark.PINKY_TIP].x * image_width\n                pinky_TipY = hand_landmarks.landmark[mp_hands.HandLandmark.PINKY_TIP].y * image_height\n                pinky_TipZ = hand_landmarks.landmark[mp_hands.HandLandmark.PINKY_TIP].z\n\n                # Draw the Skeleton\n                mp_drawing.draw_landmarks(annotated_image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n                \n            return (wristX, wristY, wristZ,\n                    thumb_CmcX, thumb_CmcY, thumb_CmcZ,\n                    thumb_McpX, thumb_McpY, thumb_McpZ,\n                    thumb_IpX, thumb_IpY, thumb_IpZ,\n                    thumb_TipX, thumb_TipY, thumb_TipZ,\n                    index_McpX, index_McpY, index_McpZ,\n                    index_PipX, index_PipY, index_PipZ,\n                    index_DipX, index_DipY, index_DipZ,\n                    index_TipX, index_TipY, index_TipZ,\n                    middle_McpX, middle_McpY, middle_McpZ,\n                    middle_PipX, middle_PipY, middle_PipZ,\n                    middle_DipX, middle_DipY, middle_DipZ,\n                    middle_TipX, middle_TipY, middle_TipZ,\n                    ring_McpX, ring_McpY, ring_McpZ,\n                    ring_PipX, ring_PipY, ring_PipZ,\n                    ring_DipX, ring_DipY, ring_DipZ,\n                    ring_TipX, ring_TipY, ring_TipZ,\n                    pinky_McpX, pinky_McpY, pinky_McpZ,\n                    pinky_PipX, pinky_PipY, pinky_PipZ,\n                    pinky_DipX, pinky_DipY, pinky_DipZ,\n                    pinky_TipX, pinky_TipY, pinky_TipZ,\n                    annotated_image)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T06:24:04.547099Z","iopub.execute_input":"2023-09-14T06:24:04.547947Z","iopub.status.idle":"2023-09-14T06:24:04.596383Z","shell.execute_reply.started":"2023-09-14T06:24:04.547901Z","shell.execute_reply":"2023-09-14T06:24:04.595376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Function to create CSV file or add dataset to the existed CSV file\ndef toCSV(filecsv, class_type,\n          wristX, wristY, wristZ,\n          thumb_CmcX, thumb_CmcY, thumb_CmcZ,\n          thumb_McpX, thumb_McpY, thumb_McpZ,\n          thumb_IpX, thumb_IpY, thumb_IpZ,\n          thumb_TipX, thumb_TipY, thumb_TipZ,\n          index_McpX, index_McpY, index_McpZ,\n          index_PipX, index_PipY, index_PipZ,\n          index_DipX, index_DipY, index_DipZ,\n          index_TipX, index_TipY, index_TipZ,\n          middle_McpX, middle_McpY, middle_McpZ,\n          middle_PipX, middle_PipY, middle_PipZ,\n          middle_DipX, middle_DipY, middle_DipZ,\n          middle_TipX, middle_TipY, middle_TipZ,\n          ring_McpX, ring_McpY, ring_McpZ,\n          ring_PipX, ring_PipY, ring_PipZ,\n          ring_DipX, ring_DipY, ring_DipZ,\n          ring_TipX, ring_TipY, ring_TipZ,\n          pinky_McpX, pinky_McpY, pinky_McpZ,\n          pinky_PipX, pinky_PipY, pinky_PipZ,\n          pinky_DipX, pinky_DipY, pinky_DipZ,\n          pinky_TipX, pinky_TipY, pinky_TipZ):\n    if os.path.isfile(filecsv):\n        #print (\"File exist thus shall write append to the file\")\n        with open(filecsv, 'a+', newline='') as file:\n            # Create a writer object from csv module\n            writer = csv.writer(file)\n            writer.writerow([class_type,\n                             wristX, wristY, wristZ,\n                             thumb_CmcX, thumb_CmcY, thumb_CmcZ,\n                             thumb_McpX, thumb_McpY, thumb_McpZ,\n                             thumb_IpX, thumb_IpY, thumb_IpZ,\n                             thumb_TipX, thumb_TipY, thumb_TipZ,\n                             index_McpX, index_McpY, index_McpZ,\n                             index_PipX, index_PipY, index_PipZ,\n                             index_DipX, index_DipY, index_DipZ,\n                             index_TipX, index_TipY, index_TipZ,\n                             middle_McpX, middle_McpY, middle_McpZ,\n                             middle_PipX, middle_PipY, middle_PipZ,\n                             middle_DipX, middle_DipY, middle_DipZ,\n                             middle_TipX, middle_TipY, middle_TipZ,\n                             ring_McpX, ring_McpY, ring_McpZ,\n                             ring_PipX, ring_PipY, ring_PipZ,\n                             ring_DipX, ring_DipY, ring_DipZ,\n                             ring_TipX, ring_TipY, ring_TipZ,\n                             pinky_McpX, pinky_McpY, pinky_McpZ,\n                             pinky_PipX, pinky_PipY, pinky_PipZ,\n                             pinky_DipX, pinky_DipY, pinky_DipZ,\n                             pinky_TipX, pinky_TipY, pinky_TipZ])\n    else:\n        #print (\"File not exist thus shall create new file as\", filecsv)\n        with open(filecsv, 'w', newline='') as file:\n            # Create a writer object from csv module\n            writer = csv.writer(file)\n            writer.writerow([\"class_type\",\n                             \"wristX\", \"wristY\", \"wristZ\",\n                             \"thumb_CmcX\", \"thumb_CmcY\", \"thumb_CmcZ\",\n                             \"thumb_McpX\", \"thumb_McpY\", \"thumb_McpZ\",\n                             \"thumb_IpX\", \"thumb_IpY\", \"thumb_IpZ\",\n                             \"thumb_TipX\", \"thumb_TipY\", \"thumb_TipZ\",\n                             \"index_McpX\", \"index_McpY\", \"index_McpZ\",\n                             \"index_PipX\", \"index_PipY\", \"index_PipZ\",\n                             \"index_DipX\", \"index_DipY\", \"index_DipZ\",\n                             \"index_TipX\", \"index_TipY\", \"index_TipZ\",\n                             \"middle_McpX\", \"middle_McpY\", \"middle_McpZ\",\n                             \"middle_PipX\", \"middle_PipY\", \"middle_PipZ\",\n                             \"middle_DipX\", \"middle_DipY\", \"middle_DipZ\",\n                             \"middle_TipX\", \"middle_TipY\", \"middle_TipZ\",\n                             \"ring_McpX\", \"ring_McpY\", \"ring_McpZ\",\n                             \"ring_PipX\", \"ring_PipY\", \"ring_PipZ\",\n                             \"ring_DipX\", \"ring_DipY\", \"ring_DipZ\",\n                             \"ring_TipX\", \"ring_TipY\", \"ring_TipZ\",\n                             \"pinky_McpX\", \"pinky_McpY\", \"pinky_McpZ\",\n                             \"pinky_PipX\", \"pinky_PipY\", \"pinky_PipZ\",\n                             \"pinky_DipX\", \"pinky_DipY\", \"pinky_DipZ\",\n                             \"pinky_TipX\", \"pinky_TipY\", \"pinky_TipZ\"])\n            writer.writerow([class_type,\n                             wristX, wristY, wristZ,\n                             thumb_CmcX, thumb_CmcY, thumb_CmcZ,\n                             thumb_McpX, thumb_McpY, thumb_McpZ,\n                             thumb_IpX, thumb_IpY, thumb_IpZ,\n                             thumb_TipX, thumb_TipY, thumb_TipZ,\n                             index_McpX, index_McpY, index_McpZ,\n                             index_PipX, index_PipY, index_PipZ,\n                             index_DipX, index_DipY, index_DipZ,\n                             index_TipX, index_TipY, index_TipZ,\n                             middle_McpX, middle_McpY, middle_McpZ,\n                             middle_PipX, middle_PipY, middle_PipZ,\n                             middle_DipX, middle_DipY, middle_DipZ,\n                             middle_TipX, middle_TipY, middle_TipZ,\n                             ring_McpX, ring_McpY, ring_McpZ,\n                             ring_PipX, ring_PipY, ring_PipZ,\n                             ring_DipX, ring_DipY, ring_DipZ,\n                             ring_TipX, ring_TipY, ring_TipZ,\n                             pinky_McpX, pinky_McpY, pinky_McpZ,\n                             pinky_PipX, pinky_PipY, pinky_PipZ,\n                             pinky_DipX, pinky_DipY, pinky_DipZ,\n                             pinky_TipX, pinky_TipY, pinky_TipZ])","metadata":{"execution":{"iopub.status.busy":"2023-09-14T06:24:52.360877Z","iopub.execute_input":"2023-09-14T06:24:52.361300Z","iopub.status.idle":"2023-09-14T06:24:52.382771Z","shell.execute_reply.started":"2023-09-14T06:24:52.361265Z","shell.execute_reply":"2023-09-14T06:24:52.381570Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract Feature for Training\n# We will using SIBI datasets version V02\npaths = \"/kaggle/input/rgb-arabic-alphabets-sign-language-dataset/RGB ArSL dataset/\"\ncsv_path = \"hands_SIBI_training.csv\"\n\nif os.path.exists(csv_path):\n    print(\"CSV File does exist, going delete before start extraction and replace it with new\")\n    os.remove(csv_path)\nelse:\n    print(\"The CSV file does not exist\", csv_path, \",Going Create after Extraction\")\n    \nfor dirlist in os.listdir(paths):\n    for root, directories, filenames in os.walk(os.path.join(paths, dirlist)):\n        print(\"Inside Folder\", dirlist, \"Consist :\", len(filenames), \"Imageset\")\n        for filename in filenames:\n            if filename.endswith(\".jpg\") or filename.endswith(\".JPG\"):\n                #print(os.path.join(root, filename), True)\n                (wristX, wristY, wristZ,\n                 thumb_CmcX, thumb_CmcY, thumb_CmcZ,\n                 thumb_McpX, thumb_McpY, thumb_McpZ,\n                 thumb_IpX, thumb_IpY, thumb_IpZ,\n                 thumb_TipX, thumb_TipY, thumb_TipZ,\n                 index_McpX, index_McpY, index_McpZ,\n                 index_PipX, index_PipY, index_PipZ,\n                 index_DipX, index_DipY, index_DipZ,\n                 index_TipX, index_TipY, index_TipZ,\n                 middle_McpX, middle_McpY, middle_McpZ,\n                 middle_PipX, middle_PipY, middle_PipZ,\n                 middle_DipX, middle_DipY, middle_DipZ,\n                 middle_TipX, middle_TipY, middle_TipZ,\n                 ring_McpX, ring_McpY, ring_McpZ,\n                 ring_PipX, ring_PipY, ring_PipZ,\n                 ring_DipX, ring_DipY, ring_DipZ,\n                 ring_TipX, ring_TipY, ring_TipZ,\n                 pinky_McpX, pinky_McpY, pinky_McpZ,\n                 pinky_PipX, pinky_PipY, pinky_PipZ,\n                 pinky_DipX, pinky_DipY, pinky_DipZ,\n                 pinky_TipX, pinky_TipY, pinky_TipZ,\n                 annotated_image) = extract_feature(os.path.join(root, filename))\n            \n                if ((not wristX == 0) and (not wristY == 0)):\n                    toCSV(csv_path, dirlist, \n                          wristX, wristY, wristZ,\n                          thumb_CmcX, thumb_CmcY, thumb_CmcZ,\n                          thumb_McpX, thumb_McpY, thumb_McpZ,\n                          thumb_IpX, thumb_IpY, thumb_IpZ,\n                          thumb_TipX, thumb_TipY, thumb_TipZ,\n                          index_McpX, index_McpY, index_McpZ,\n                          index_PipX, index_PipY, index_PipZ,\n                          index_DipX, index_DipY, index_DipZ,\n                          index_TipX, index_TipY, index_TipZ,\n                          middle_McpX, middle_McpY, middle_McpZ,\n                          middle_PipX, middle_PipY, middle_PipZ,\n                          middle_DipX, middle_DipY, middle_DipZ,\n                          middle_TipX, middle_TipY, middle_TipZ,\n                          ring_McpX, ring_McpY, ring_McpZ,\n                          ring_PipX, ring_PipY, ring_PipZ,\n                          ring_DipX, ring_DipY, ring_DipZ,\n                          ring_TipX, ring_TipY, ring_TipZ,\n                          pinky_McpX, pinky_McpY, pinky_McpZ,\n                          pinky_PipX, pinky_PipY, pinky_PipZ,\n                          pinky_DipX, pinky_DipY, pinky_DipZ,\n                          pinky_TipX, pinky_TipY, pinky_TipZ,)\n                \n                else :\n                    print(os.path.join(root, filename), \"Hand does not have landmarks\")\n\nprint(\"===================Feature Extraction for TRAINING is Completed===================\")\n                ","metadata":{"execution":{"iopub.status.busy":"2023-09-14T06:25:08.776848Z","iopub.execute_input":"2023-09-14T06:25:08.777263Z","iopub.status.idle":"2023-09-14T06:53:30.276060Z","shell.execute_reply.started":"2023-09-14T06:25:08.777229Z","shell.execute_reply":"2023-09-14T06:53:30.274898Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Extract Feature for Validation\n# # We will using SIBI datasets version V02\n# paths = \"/kaggle/input/datasets-lemlitbang-sibi-alphabets/SIBI_datasets_LEMLITBANG_SIBI_R_90.10_V02/SIBI_datasets_LEMLITBANG_SIBI_R_90.10_V02/validation/\"\n# csv_path = \"hands_SIBI_validation.csv\"\n\n# if os.path.exists(csv_path):\n#     print(\"CSV File does exist, going delete before start extraction and replace it with new\")\n#     os.remove(csv_path)\n# else:\n#     print(\"The CSV file does not exist\", csv_path, \",Going Create after Extraction\")\n    \n# for dirlist in os.listdir(paths):\n#     for root, directories, filenames in os.walk(os.path.join(paths, dirlist)):\n#         print(\"Inside Folder\", dirlist, \"Consist :\", len(filenames), \"Imageset\")\n#         for filename in filenames:\n#             if filename.endswith(\".jpg\") or filename.endswith(\".JPG\"):\n#                 #print(os.path.join(root, filename), True)\n#                 (wristX, wristY, wristZ,\n#                  thumb_CmcX, thumb_CmcY, thumb_CmcZ,\n#                  thumb_McpX, thumb_McpY, thumb_McpZ,\n#                  thumb_IpX, thumb_IpY, thumb_IpZ,\n#                  thumb_TipX, thumb_TipY, thumb_TipZ,\n#                  index_McpX, index_McpY, index_McpZ,\n#                  index_PipX, index_PipY, index_PipZ,\n#                  index_DipX, index_DipY, index_DipZ,\n#                  index_TipX, index_TipY, index_TipZ,\n#                  middle_McpX, middle_McpY, middle_McpZ,\n#                  middle_PipX, middle_PipY, middle_PipZ,\n#                  middle_DipX, middle_DipY, middle_DipZ,\n#                  middle_TipX, middle_TipY, middle_TipZ,\n#                  ring_McpX, ring_McpY, ring_McpZ,\n#                  ring_PipX, ring_PipY, ring_PipZ,\n#                  ring_DipX, ring_DipY, ring_DipZ,\n#                  ring_TipX, ring_TipY, ring_TipZ,\n#                  pinky_McpX, pinky_McpY, pinky_McpZ,\n#                  pinky_PipX, pinky_PipY, pinky_PipZ,\n#                  pinky_DipX, pinky_DipY, pinky_DipZ,\n#                  pinky_TipX, pinky_TipY, pinky_TipZ,\n#                  annotated_image) = extract_feature(os.path.join(root, filename))\n            \n#                 if ((not wristX == 0) and (not wristY == 0)):\n#                     toCSV(csv_path, dirlist, \n#                           wristX, wristY, wristZ,\n#                           thumb_CmcX, thumb_CmcY, thumb_CmcZ,\n#                           thumb_McpX, thumb_McpY, thumb_McpZ,\n#                           thumb_IpX, thumb_IpY, thumb_IpZ,\n#                           thumb_TipX, thumb_TipY, thumb_TipZ,\n#                           index_McpX, index_McpY, index_McpZ,\n#                           index_PipX, index_PipY, index_PipZ,\n#                           index_DipX, index_DipY, index_DipZ,\n#                           index_TipX, index_TipY, index_TipZ,\n#                           middle_McpX, middle_McpY, middle_McpZ,\n#                           middle_PipX, middle_PipY, middle_PipZ,\n#                           middle_DipX, middle_DipY, middle_DipZ,\n#                           middle_TipX, middle_TipY, middle_TipZ,\n#                           ring_McpX, ring_McpY, ring_McpZ,\n#                           ring_PipX, ring_PipY, ring_PipZ,\n#                           ring_DipX, ring_DipY, ring_DipZ,\n#                           ring_TipX, ring_TipY, ring_TipZ,\n#                           pinky_McpX, pinky_McpY, pinky_McpZ,\n#                           pinky_PipX, pinky_PipY, pinky_PipZ,\n#                           pinky_DipX, pinky_DipY, pinky_DipZ,\n#                           pinky_TipX, pinky_TipY, pinky_TipZ,)\n                \n#                 else :\n#                     print(os.path.join(root, filename), \"Hand does not have landmarks\")\n                \n# print(\"===================Feature Extraction for VALIDATION is Completed===================\")","metadata":{"execution":{"iopub.status.busy":"2021-08-24T14:37:03.617731Z","iopub.execute_input":"2021-08-24T14:37:03.618052Z","iopub.status.idle":"2021-08-24T14:37:37.089819Z","shell.execute_reply.started":"2021-08-24T14:37:03.618017Z","shell.execute_reply":"2021-08-24T14:37:37.087703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read CSV file for Training the model using Pandas\ndf_train = pd.read_csv(\"hands_SIBI_training.csv\", header=0)\n\n# First we must sort the values of the dataset according to the Alphabets\ndf_train = df_train.sort_values(by=[\"class_type\"])\n\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-14T09:13:13.949448Z","iopub.execute_input":"2023-09-14T09:13:13.949842Z","iopub.status.idle":"2023-09-14T09:13:14.159029Z","shell.execute_reply.started":"2023-09-14T09:13:13.949811Z","shell.execute_reply":"2023-09-14T09:13:14.157807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(df_train[\"class_type\"].unique())","metadata":{"execution":{"iopub.status.busy":"2023-09-14T09:13:14.436209Z","iopub.execute_input":"2023-09-14T09:13:14.436622Z","iopub.status.idle":"2023-09-14T09:13:14.445623Z","shell.execute_reply.started":"2023-09-14T09:13:14.436589Z","shell.execute_reply":"2023-09-14T09:13:14.444330Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Read CSV file for Validation or Testing the Model using Pandas\n# df_test = pd.read_csv(\"hands_SIBI_validation.csv\", header=0)\n\n# # First we must sort the values of the dataset according to the Alphabets\n# df_test = df_test.sort_values(by=[\"class_type\"])\n\n# df_test","metadata":{"execution":{"iopub.status.busy":"2023-09-14T09:13:14.462383Z","iopub.execute_input":"2023-09-14T09:13:14.462832Z","iopub.status.idle":"2023-09-14T09:13:14.468563Z","shell.execute_reply.started":"2023-09-14T09:13:14.462795Z","shell.execute_reply":"2023-09-14T09:13:14.467238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Put Categorical using Pandas\ndf_train[\"class_type\"] = pd.Categorical(df_train[\"class_type\"])\ndf_train[\"class_type\"] = df_train.class_type.cat.codes\n\n# df_test[\"class_type\"] = pd.Categorical(df_test[\"class_type\"])\n# df_test[\"class_type\"] = df_test.class_type.cat.codes","metadata":{"execution":{"iopub.status.busy":"2023-09-14T09:13:14.508347Z","iopub.execute_input":"2023-09-14T09:13:14.508760Z","iopub.status.idle":"2023-09-14T09:13:14.518094Z","shell.execute_reply.started":"2023-09-14T09:13:14.508726Z","shell.execute_reply":"2023-09-14T09:13:14.516831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df_train.drop('class_type', axis=1)\ny=df_train[\"class_type\"]","metadata":{"execution":{"iopub.status.busy":"2023-09-14T09:13:14.536265Z","iopub.execute_input":"2023-09-14T09:13:14.536670Z","iopub.status.idle":"2023-09-14T09:13:14.544433Z","shell.execute_reply.started":"2023-09-14T09:13:14.536638Z","shell.execute_reply":"2023-09-14T09:13:14.542947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Assuming X is your data and y are your labels\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-14T09:13:14.576088Z","iopub.execute_input":"2023-09-14T09:13:14.576522Z","iopub.status.idle":"2023-09-14T09:13:14.588232Z","shell.execute_reply.started":"2023-09-14T09:13:14.576488Z","shell.execute_reply":"2023-09-14T09:13:14.586717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nx_train = np.array(x_train)\nx_test = np.array(x_test)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-14T09:13:14.604296Z","iopub.execute_input":"2023-09-14T09:13:14.604709Z","iopub.status.idle":"2023-09-14T09:13:14.611657Z","shell.execute_reply.started":"2023-09-14T09:13:14.604677Z","shell.execute_reply":"2023-09-14T09:13:14.610467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check Array Shape before transformation\nprint(x_train.shape)\nprint(x_test.shape)\n\n# Since the array shape is 1x1, we must turn it into 1x10x1 so we can feed it into the model\nx_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\nx_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))\n\n# Check Array Shape after transformation\nprint(x_train.shape)\nprint(x_test.shape)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T09:13:14.645698Z","iopub.execute_input":"2023-09-14T09:13:14.646055Z","iopub.status.idle":"2023-09-14T09:13:14.654150Z","shell.execute_reply.started":"2023-09-14T09:13:14.646027Z","shell.execute_reply":"2023-09-14T09:13:14.652855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check sample train and test features\nprint(x_train[0])\nprint(x_test[7])","metadata":{"execution":{"iopub.status.busy":"2023-09-14T09:13:14.672651Z","iopub.execute_input":"2023-09-14T09:13:14.673082Z","iopub.status.idle":"2023-09-14T09:13:14.682034Z","shell.execute_reply.started":"2023-09-14T09:13:14.673048Z","shell.execute_reply":"2023-09-14T09:13:14.680803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Number of classes according standard Indonesian Language Alphabets\nnum_classes = 31\n\n# Using the Keras.Utils to put the label categorically \ny_train = to_categorical(y_train, num_classes)\ny_test = to_categorical(y_test, num_classes)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T09:13:14.925365Z","iopub.execute_input":"2023-09-14T09:13:14.925801Z","iopub.status.idle":"2023-09-14T09:13:14.932397Z","shell.execute_reply.started":"2023-09-14T09:13:14.925764Z","shell.execute_reply":"2023-09-14T09:13:14.931113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train.shape[1:3]","metadata":{"execution":{"iopub.status.busy":"2023-09-14T09:13:15.067264Z","iopub.execute_input":"2023-09-14T09:13:15.067644Z","iopub.status.idle":"2023-09-14T09:13:15.073620Z","shell.execute_reply.started":"2023-09-14T09:13:15.067615Z","shell.execute_reply":"2023-09-14T09:13:15.072787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from tensorflow.keras import regularizers\n# from tensorflow.keras.optimizers import Adam\n\n# # Defining the model\n# model = tf.keras.models.Sequential([\n#     # First Convolutional layer with 32 filters\n#     tf.keras.layers.Conv1D(filters=32, kernel_size=3, strides=1, padding=\"causal\", activation=\"relu\", input_shape=x_train.shape[1:3]),\n    \n#     # Max Pooling layer with pool size of 2\n#     tf.keras.layers.MaxPooling1D(pool_size=2),\n    \n#     # Second Convolutional layer with 64 filters\n#     tf.keras.layers.Conv1D(filters=64, kernel_size=3, strides=1, padding=\"causal\", activation=\"relu\"),\n    \n#     # Max Pooling layer\n#     tf.keras.layers.MaxPooling1D(pool_size=2),\n    \n#     # Third Convolutional layer with 128 filters\n#     tf.keras.layers.Conv1D(filters=128, kernel_size=3, strides=1, padding=\"causal\", activation=\"relu\"),\n    \n#     # Max Pooling layer\n#     tf.keras.layers.MaxPooling1D(pool_size=2),\n    \n#     # Dropout layer with dropout rate of 0.5 to prevent overfitting\n#     tf.keras.layers.Dropout(rate=0.5),\n    \n#     # Flatten the results to feed into a DNN\n#     tf.keras.layers.Flatten(),\n    \n#     # Dense hidden layer with 256 neurons and 'relu' activation function\n#     tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)),  # Added L2 regularization\n    \n#     # Output layer with 'num_classes' neurons (one for each class) and 'softmax' activation function for multi-class classification\n#     tf.keras.layers.Dense(num_classes, activation='softmax')])\n\n\n# # Reduced learning rate\n# optimizer = Adam(learning_rate=0.001)\n\n# # Compiling the model with 'categorical_crossentropy' loss function (for multi-class classification), 'adam' optimizer and accuracy as the metric\n# model.compile(loss = 'categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n\n\n# # Printing the summary of the model\n# model.summary()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-14T09:13:15.540482Z","iopub.execute_input":"2023-09-14T09:13:15.540849Z","iopub.status.idle":"2023-09-14T09:13:15.548270Z","shell.execute_reply.started":"2023-09-14T09:13:15.540821Z","shell.execute_reply":"2023-09-14T09:13:15.546968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# history=model.fit(x_train, y_train, epochs=100, batch_size=32, validation_data=(x_test, y_test))","metadata":{"execution":{"iopub.status.busy":"2023-09-14T09:13:15.997062Z","iopub.execute_input":"2023-09-14T09:13:15.997835Z","iopub.status.idle":"2023-09-14T09:13:16.003880Z","shell.execute_reply.started":"2023-09-14T09:13:15.997797Z","shell.execute_reply":"2023-09-14T09:13:16.002353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from tensorflow.keras import regularizers\n# from tensorflow.keras.optimizers import Adam\n\n# # Defining the model\n# model_2 = tf.keras.models.Sequential([\n#     # First Convolutional layer with 32 filters\n#     tf.keras.layers.Conv1D(filters=32, kernel_size=3, strides=1, padding=\"causal\", activation=\"relu\", input_shape=x_train.shape[1:3]),\n    \n#     # Max Pooling layer with pool size of 2\n#     tf.keras.layers.MaxPooling1D(pool_size=2),\n    \n#     # LSTM layer with 64 units\n#     tf.keras.layers.LSTM(64, return_sequences=True),\n    \n#     # Second Convolutional layer with 64 filters\n#     tf.keras.layers.Conv1D(filters=64, kernel_size=3, strides=1, padding=\"causal\", activation=\"relu\"),\n    \n#     # Max Pooling layer\n#     tf.keras.layers.MaxPooling1D(pool_size=2),\n    \n#     # LSTM layer with 128 units\n#     tf.keras.layers.LSTM(128),\n    \n#     # Dropout layer with dropout rate of 0.5 to prevent overfitting\n#     tf.keras.layers.Dropout(rate=0.5),\n    \n#     # Dense hidden layer with 256 neurons and 'relu' activation function\n#     tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)),  # Added L2 regularization\n    \n#     # Output layer with 'num_classes' neurons (one for each class) and 'softmax' activation function for multi-class classification\n#     tf.keras.layers.Dense(num_classes, activation='softmax')])\n\n# # Add early stopping\n# callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n\n# # Reduced learning rate\n# optimizer = Adam(learning_rate=0.001)\n\n# # Compiling the model with 'categorical_crossentropy' loss function (for multi-class classification), 'adam' optimizer and accuracy as the metric\n# model_2.compile(loss = 'categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n\n\n\n# # Printing the summary of the model\n# model_2.summary()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-14T09:13:16.364989Z","iopub.execute_input":"2023-09-14T09:13:16.365405Z","iopub.status.idle":"2023-09-14T09:13:16.372912Z","shell.execute_reply.started":"2023-09-14T09:13:16.365373Z","shell.execute_reply":"2023-09-14T09:13:16.371370Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_classes","metadata":{"execution":{"iopub.status.busy":"2023-09-14T09:13:16.849714Z","iopub.execute_input":"2023-09-14T09:13:16.850170Z","iopub.status.idle":"2023-09-14T09:13:16.857467Z","shell.execute_reply.started":"2023-09-14T09:13:16.850108Z","shell.execute_reply":"2023-09-14T09:13:16.856079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n\n# Name of the model\nmodel_name = \"best_model.h5\"\n\n# ModelCheckpoint callback\ncheckpoint = ModelCheckpoint(model_name,\n                             monitor=\"val_loss\",\n                             mode=\"min\",\n                             save_best_only=True,\n                             verbose=1)\n\n# EarlyStopping callback\nearlystopping = EarlyStopping(monitor='val_loss',\n                              min_delta=0,\n                              patience=3,  # Reduced patience to 3\n                              verbose=1,\n                              restore_best_weights=True)\n\n# ReduceLROnPlateau callback\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_loss',\n                                            patience=2,  # Reduced patience to 2\n                                            verbose=1,\n                                            factor=0.5,  # Changed factor to 0.5\n                                            min_lr=0.00001)  # Added minimum learning rate","metadata":{"execution":{"iopub.status.busy":"2023-09-14T09:13:17.220229Z","iopub.execute_input":"2023-09-14T09:13:17.220631Z","iopub.status.idle":"2023-09-14T09:13:17.229008Z","shell.execute_reply.started":"2023-09-14T09:13:17.220602Z","shell.execute_reply":"2023-09-14T09:13:17.227651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import regularizers\nfrom tensorflow.keras.optimizers import Adam\n\n# Defining the model\nmodel = tf.keras.models.Sequential([\n    # First Convolutional layer with 32 filters\n    tf.keras.layers.Conv1D(filters=32, kernel_size=3, strides=1, padding=\"causal\", activation=\"relu\", input_shape=x_train.shape[1:3]),\n    \n    # Max Pooling layer with pool size of 2\n    tf.keras.layers.MaxPooling1D(pool_size=2),\n    \n    # Second Convolutional layer with 64 filters\n    tf.keras.layers.Conv1D(filters=64, kernel_size=3, strides=1, padding=\"causal\", activation=\"relu\"),\n    \n    # Max Pooling layer\n    tf.keras.layers.MaxPooling1D(pool_size=2),\n    \n    # Third Convolutional layer with 128 filters\n    tf.keras.layers.Conv1D(filters=128, kernel_size=3, strides=1, padding=\"causal\", activation=\"relu\"),\n    \n    # Max Pooling layer\n    tf.keras.layers.MaxPooling1D(pool_size=2),\n    \n    # Dropout layer with dropout rate of 0.5 to prevent overfitting\n    tf.keras.layers.Dropout(rate=0.5),\n    \n    # Flatten the results to feed into a DNN\n    tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.01)),  # Added L2 regularization\n\n    # Dense hidden layer with 256 neurons and 'relu' activation function\n    tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)),  # Added L2 regularization\n    \n    # Output layer with 'num_classes' neurons (one for each class) and 'softmax' activation function for multi-class classification\n    tf.keras.layers.Dense(num_classes, activation='softmax')])\n\n\n# Reduced learning rate\noptimizer = Adam(learning_rate=0.001)\n\n# Compiling the model with 'categorical_crossentropy' loss function (for multi-class classification), 'adam' optimizer and accuracy as the metric\nmodel.compile(loss = 'categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n\n# Training the model with early stopping callback\n\n# Printing the summary of the model\nmodel.summary()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-14T09:13:17.791407Z","iopub.execute_input":"2023-09-14T09:13:17.791845Z","iopub.status.idle":"2023-09-14T09:13:17.974170Z","shell.execute_reply.started":"2023-09-14T09:13:17.791804Z","shell.execute_reply":"2023-09-14T09:13:17.972870Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history=model.fit(x_train, y_train, epochs=100, batch_size=32, validation_data=(x_test, y_test),callbacks=[checkpoint,earlystopping,learning_rate_reduction])","metadata":{"execution":{"iopub.status.busy":"2023-09-14T09:13:18.582009Z","iopub.execute_input":"2023-09-14T09:13:18.582425Z","iopub.status.idle":"2023-09-14T09:16:53.896274Z","shell.execute_reply.started":"2023-09-14T09:13:18.582393Z","shell.execute_reply":"2023-09-14T09:16:53.895229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # One Dimensional Convolutional Neural Network model, Train will be feed to 1 Dimension Convolutional Neural Network\n# model = tf.keras.models.Sequential([\n#     tf.keras.layers.Conv1D(filters=32, kernel_size=5, strides=1, padding=\"causal\", activation=\"relu\", input_shape=x_train.shape[1:3]),\n#     tf.keras.layers.Conv1D(filters=32, kernel_size=5, strides=1, padding=\"causal\", activation=\"relu\"),\n#     tf.keras.layers.MaxPooling1D(pool_size=2),\n#     tf.keras.layers.Conv1D(filters=64, kernel_size=5, strides=1, padding=\"causal\", activation=\"relu\"),\n#     tf.keras.layers.Conv1D(filters=64, kernel_size=5, strides=1, padding=\"causal\", activation=\"relu\"),\n#     tf.keras.layers.MaxPooling1D(pool_size=2),\n#     tf.keras.layers.Conv1D(filters=128, kernel_size=5, strides=1, padding=\"causal\", activation=\"relu\"),\n#     tf.keras.layers.Conv1D(filters=128, kernel_size=5, strides=1, padding=\"causal\", activation=\"relu\"),\n#     tf.keras.layers.MaxPooling1D(pool_size=2),\n#     tf.keras.layers.Conv1D(filters=256, kernel_size=5, strides=1, padding=\"causal\", activation=\"relu\"),\n#     tf.keras.layers.Conv1D(filters=256, kernel_size=5, strides=1, padding=\"causal\", activation=\"relu\"),\n#     tf.keras.layers.MaxPooling1D(pool_size=2),\n#     tf.keras.layers.Dropout(rate=0.2),\n#     # Flatten the results to feed into a DNN\n#     tf.keras.layers.Flatten(),\n#     # 512 neuron hidden layer\n#     tf.keras.layers.Dense(512, activation='relu'), \n#     tf.keras.layers.Dense(num_classes, activation='softmax')])\n\n# model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n# model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-09-14T09:16:56.743252Z","iopub.execute_input":"2023-09-14T09:16:56.743669Z","iopub.status.idle":"2023-09-14T09:16:56.750747Z","shell.execute_reply.started":"2023-09-14T09:16:56.743635Z","shell.execute_reply":"2023-09-14T09:16:56.749322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# history=model.fit(x_train, y_train, epochs=100, batch_size=32, validation_data=(x_test, y_test))","metadata":{"execution":{"iopub.status.busy":"2023-09-14T09:16:57.397002Z","iopub.execute_input":"2023-09-14T09:16:57.397422Z","iopub.status.idle":"2023-09-14T09:16:57.402425Z","shell.execute_reply.started":"2023-09-14T09:16:57.397389Z","shell.execute_reply":"2023-09-14T09:16:57.401233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,8))\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-14T09:16:57.643385Z","iopub.execute_input":"2023-09-14T09:16:57.643812Z","iopub.status.idle":"2023-09-14T09:16:58.084563Z","shell.execute_reply.started":"2023-09-14T09:16:57.643779Z","shell.execute_reply":"2023-09-14T09:16:58.083285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,8))\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-14T09:16:59.263437Z","iopub.execute_input":"2023-09-14T09:16:59.263839Z","iopub.status.idle":"2023-09-14T09:16:59.659806Z","shell.execute_reply.started":"2023-09-14T09:16:59.263810Z","shell.execute_reply":"2023-09-14T09:16:59.658568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Saving the model into H5 system file\nsave_model = \"model_arabic.h5\"\nmodel.save(save_model)\nprint(\"Model Saved into\", save_model)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T09:17:27.769683Z","iopub.execute_input":"2023-09-14T09:17:27.770417Z","iopub.status.idle":"2023-09-14T09:17:27.823931Z","shell.execute_reply.started":"2023-09-14T09:17:27.770375Z","shell.execute_reply":"2023-09-14T09:17:27.822639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test = np.argmax(y_test,axis=1)\ny_pred = model.predict(x_test)\ny_pred = np.argmax(y_pred,axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T09:17:28.729002Z","iopub.execute_input":"2023-09-14T09:17:28.730015Z","iopub.status.idle":"2023-09-14T09:17:28.923417Z","shell.execute_reply.started":"2023-09-14T09:17:28.729976Z","shell.execute_reply":"2023-09-14T09:17:28.921736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,10))\nsns.heatmap(confusion_matrix(y_test,y_pred),annot=True,fmt='.3g',cmap='viridis')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-14T09:17:29.058423Z","iopub.execute_input":"2023-09-14T09:17:29.058834Z","iopub.status.idle":"2023-09-14T09:17:31.471897Z","shell.execute_reply.started":"2023-09-14T09:17:29.058801Z","shell.execute_reply":"2023-09-14T09:17:31.470671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n# Convert the model.\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\ntflite_model = converter.convert()\n\n# Save the model.\nwith open('model_arabic.tflite', 'wb') as f:\n  f.write(tflite_model)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T09:17:36.269553Z","iopub.execute_input":"2023-09-14T09:17:36.269976Z","iopub.status.idle":"2023-09-14T09:17:39.767827Z","shell.execute_reply.started":"2023-09-14T09:17:36.269942Z","shell.execute_reply":"2023-09-14T09:17:39.766636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Hard Encode for the Prediction\nclasses={'Ain': 0, 'Al': 1, 'Alef': 2, 'Beh': 3, 'Dad': 4, 'Dal': 5, 'Feh': 6, 'Ghain': 7, 'Hah': 8, 'Heh': 9, 'Jeem': 10, 'Kaf': 11, 'Khah': 12, 'Laa': 13, 'Lam': 14, 'Meem': 15, 'Noon': 16, 'Qaf': 17, 'Reh': 18, 'Sad': 19, 'Seen': 20, 'Sheen': 21, 'Tah': 22, 'Teh': 23, 'Teh_Marbuta': 24, 'Thal': 25, 'Theh': 26, 'Waw': 27, 'Yeh': 28, 'Zah': 29, 'Zain': 30}\n","metadata":{"execution":{"iopub.status.busy":"2023-09-14T09:17:45.919500Z","iopub.execute_input":"2023-09-14T09:17:45.919917Z","iopub.status.idle":"2023-09-14T09:17:45.928104Z","shell.execute_reply.started":"2023-09-14T09:17:45.919884Z","shell.execute_reply":"2023-09-14T09:17:45.926977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2","metadata":{"execution":{"iopub.status.busy":"2023-09-14T09:17:46.304418Z","iopub.execute_input":"2023-09-14T09:17:46.304842Z","iopub.status.idle":"2023-09-14T09:17:46.310263Z","shell.execute_reply.started":"2023-09-14T09:17:46.304809Z","shell.execute_reply":"2023-09-14T09:17:46.309174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Directly from Imageset Dataset Testing\n#Load Image and do Feature Extraction\npath_to_image = \"/kaggle/input/rgb-arabic-alphabets-sign-language-dataset/RGB ArSL dataset/Seen/Seen_0.jpg\"\n\n(wristX, wristY, wristZ,\n thumb_CmcX, thumb_CmcY, thumb_CmcZ,\n thumb_McpX, thumb_McpY, thumb_McpZ,\n thumb_IpX, thumb_IpY, thumb_IpZ,\n thumb_TipX, thumb_TipY, thumb_TipZ,\n index_McpX, index_McpY, index_McpZ,\n index_PipX, index_PipY, index_PipZ,\n index_DipX, index_DipY, index_DipZ,\n index_TipX, index_TipY, index_TipZ,\n middle_McpX, middle_McpY, middle_McpZ,\n middle_PipX, middle_PipY, middle_PipZ,\n middle_DipX, middle_DipY, middle_DipZ,\n middle_TipX, middle_TipY, middle_TipZ,\n ring_McpX, ring_McpY, ring_McpZ,\n ring_PipX, ring_PipY, ring_PipZ,\n ring_DipX, ring_DipY, ring_DipZ,\n ring_TipX, ring_TipY, ring_TipZ,\n pinky_McpX, pinky_McpY, pinky_McpZ,\n pinky_PipX, pinky_PipY, pinky_PipZ,\n pinky_DipX, pinky_DipY, pinky_DipZ,\n pinky_TipX, pinky_TipY, pinky_TipZ,\n output_IMG) = extract_feature(path_to_image)\n\n#print(wristX, wristY,\n#      thumb_CmcX, thumb_CmcY, thumb_McpX, thumb_McpY, thumb_IpX, thumb_IpY, thumb_TipX, thumb_TipY,\n#      index_McpX, index_McpY, index_PipX, index_PipY, index_DipX, index_DipY, index_TipX, index_TipY,\n#      middle_McpX, middle_McpY, middle_PipX, middle_PipY, middle_DipX, middle_DipY, middle_TipX, middle_TipY,\n#      ring_McpX, ring_McpY, ring_PipX, ring_PipY, ring_DipX, ring_DipY, ring_TipX, ring_TipY,\n#      pinky_McpX, pinky_McpY, pinky_PipX, pinky_PipY, pinky_DipX, pinky_DipY, pinky_TipX, pinky_TipY)\nplt.axis(\"on\")\nplt.imshow(cv.cvtColor(output_IMG, cv.COLOR_BGR2RGB))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-14T09:18:08.054804Z","iopub.execute_input":"2023-09-14T09:18:08.055225Z","iopub.status.idle":"2023-09-14T09:18:09.657484Z","shell.execute_reply.started":"2023-09-14T09:18:08.055190Z","shell.execute_reply":"2023-09-14T09:18:09.656343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Shape the image features into an 1x3 array.\ninput_IMG = np.array([[[wristX], [wristY], [wristZ],\n                     [thumb_CmcX], [thumb_CmcY], [thumb_CmcZ],\n                     [thumb_McpX], [thumb_McpY], [thumb_McpZ],\n                     [thumb_IpX], [thumb_IpY], [thumb_IpZ],\n                     [thumb_TipX], [thumb_TipY], [thumb_TipZ],\n                     [index_McpX], [index_McpY], [index_McpZ],\n                     [index_PipX], [index_PipY], [index_PipZ],\n                     [index_DipX], [index_DipY], [index_DipZ],\n                     [index_TipX], [index_TipY], [index_TipZ],\n                     [middle_McpX], [middle_McpY], [middle_McpZ],\n                     [middle_PipX], [middle_PipY], [middle_PipZ],\n                     [middle_DipX], [middle_DipY], [middle_DipZ],\n                     [middle_TipX], [middle_TipY], [middle_TipZ],\n                     [ring_McpX], [ring_McpY], [ring_McpZ],\n                     [ring_PipX], [ring_PipY], [ring_PipZ],\n                     [ring_DipX], [ring_DipY], [ring_DipZ],\n                     [ring_TipX], [ring_TipY], [ring_TipZ],\n                     [pinky_McpX], [pinky_McpY], [pinky_McpZ],\n                     [pinky_PipX], [pinky_PipY], [pinky_PipZ],\n                     [pinky_DipX], [pinky_DipY], [pinky_DipZ],\n                     [pinky_TipX], [pinky_TipY], [pinky_TipZ]]])\n\nprint(input_IMG.shape)\nprint(input_IMG)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T09:18:11.587880Z","iopub.execute_input":"2023-09-14T09:18:11.588628Z","iopub.status.idle":"2023-09-14T09:18:11.602153Z","shell.execute_reply.started":"2023-09-14T09:18:11.588590Z","shell.execute_reply":"2023-09-14T09:18:11.600751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Print the Prediction\nprint(model.predict(input_IMG))","metadata":{"execution":{"iopub.status.busy":"2023-09-14T09:18:12.337658Z","iopub.execute_input":"2023-09-14T09:18:12.338021Z","iopub.status.idle":"2023-09-14T09:18:12.417845Z","shell.execute_reply.started":"2023-09-14T09:18:12.337993Z","shell.execute_reply":"2023-09-14T09:18:12.416544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Print prediction using defined Classes\npredictions = model.predict(input_IMG)\npredictions=np.argmax(predictions,axis=1)\nfor alphabets, values in classes.items():\n    if values == predictions[0] :\n        print(\"Possible Alphabet according to the input : \", alphabets)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T09:18:13.066040Z","iopub.execute_input":"2023-09-14T09:18:13.066434Z","iopub.status.idle":"2023-09-14T09:18:13.145525Z","shell.execute_reply.started":"2023-09-14T09:18:13.066403Z","shell.execute_reply":"2023-09-14T09:18:13.144269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}