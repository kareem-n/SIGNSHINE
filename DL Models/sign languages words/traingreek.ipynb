{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6887367,"sourceType":"datasetVersion","datasetId":3956448}],"dockerImageVersionId":30558,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-05T07:11:58.464479Z","iopub.execute_input":"2023-11-05T07:11:58.465445Z","iopub.status.idle":"2023-11-05T07:11:58.841770Z","shell.execute_reply.started":"2023-11-05T07:11:58.465404Z","shell.execute_reply":"2023-11-05T07:11:58.840771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install mediapipe","metadata":{"execution":{"iopub.status.busy":"2023-11-05T07:12:13.821476Z","iopub.execute_input":"2023-11-05T07:12:13.821920Z","iopub.status.idle":"2023-11-05T07:12:29.100771Z","shell.execute_reply.started":"2023-11-05T07:12:13.821892Z","shell.execute_reply":"2023-11-05T07:12:29.099802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import mediapipe as mp\nimport tensorflow as tf\nimport keras\nimport numpy as np\nimport pandas as pd\nimport os\nimport shutil\nimport datetime as dt\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nimport glob\nimport cv2\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Bidirectional, LSTM, Dense\nfrom keras.callbacks import EarlyStopping\nfrom sklearn.metrics import multilabel_confusion_matrix, accuracy_score\nfrom sklearn.model_selection import train_test_split\nnp.random.seed(42)","metadata":{"execution":{"iopub.status.busy":"2023-11-05T07:13:04.358352Z","iopub.execute_input":"2023-11-05T07:13:04.358687Z","iopub.status.idle":"2023-11-05T07:13:13.612158Z","shell.execute_reply.started":"2023-11-05T07:13:04.358663Z","shell.execute_reply":"2023-11-05T07:13:13.611315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mediapipe_detection(image, model):\n    if image is None:\n        raise ValueError(f\"Failed to load image: {image}\")\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n    image.flags.writeable = False                  # Image is no longer writeable\n    results = model.process(image)                 # Make prediction\n    image.flags.writeable = True                   # Image is now writeable\n    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n    return image, results","metadata":{"execution":{"iopub.status.busy":"2023-11-05T07:13:21.714402Z","iopub.execute_input":"2023-11-05T07:13:21.715170Z","iopub.status.idle":"2023-11-05T07:13:21.722318Z","shell.execute_reply.started":"2023-11-05T07:13:21.715135Z","shell.execute_reply":"2023-11-05T07:13:21.720876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a Holistic object to detect pose, face, and hands keypoints\nmp_holistic = mp.solutions.holistic\n\n# Drawing utilities\nmp_drawing = mp.solutions.drawing_utils","metadata":{"execution":{"iopub.status.busy":"2023-11-05T07:13:29.144447Z","iopub.execute_input":"2023-11-05T07:13:29.144874Z","iopub.status.idle":"2023-11-05T07:13:29.149594Z","shell.execute_reply.started":"2023-11-05T07:13:29.144839Z","shell.execute_reply":"2023-11-05T07:13:29.148649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_keypoint_arrays(path,split):\n\n    os.makedirs('/kaggle/working/Dataset/npy_arrays',exist_ok = True)\n    os.makedirs(f'/kaggle/working/Dataset/npy_arrays/{split}',exist_ok = True)\n    working_path = f'/kaggle/working/Dataset/npy_arrays/{split}'\n    words_folder = os.path.join(path, split)\n    selected_words1=[]\n    for words1 in selected_words:\n        npy_fold = os.listdir(os.path.join(working_path))\n#         if words1 not in npy_fold:\n        selected_words1.append(words1)\n    i=1\n    # Loop through all the subfolders in the folder\n    for word in tqdm(selected_words1):\n        npy_fold = os.listdir(os.path.join(working_path))\n#         if word not in npy_fold:\n        video_files = os.listdir(os.path.join(words_folder, word))\n          # Loop through the video files\n        \n        for video_file in video_files:\n            if '(1)' in video_file:\n                    # Open the video file\n                Ovideo = os.listdir(os.path.join(words_folder, word, video_file))\n                numbvideo = [f'{f.split(\"/\")[-1].split(\".\")[0].split(\"e\")[-1]}' for f in Ovideo]\n                numbvideo.sort(key = int)\n                pat = os.path.join(words_folder, word, video_file)\n                if '(1)(1)' in video_file:\n                    continue\n                else:\n                    video = [pat+f'/frame{f}.jpg.jpg' for f in numbvideo]\n                # Initialize the list of keypoints for this video\n                pose_keypoints, lh_keypoints, rh_keypoints = [], [], []\n                with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n                  # Loop through the video frames\n                  for frame in video:\n                      # Perform any necessary preprocessing on the frame (e.g., resizing, normalization)\n                    frame = os.path.join(words_folder, word, video_file,frame)\n                    frame = cv2.imread(frame)\n        #                 frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\n                      # Normalize pixel values to the range [0, 1]\n                    # Make detections\n                    if frame is not None:\n                        image, results = mediapipe_detection(frame, holistic)\n\n                        # Extract keypoints\n                        pose, lh, rh = extract_keypoints(results)\n                        # Add the keypoints to the list for this video\n                        pose_keypoints.append(pose)\n                        lh_keypoints.append(lh)\n                        rh_keypoints.append(rh)           \n                        # Save the keypoints for this video to a numpy array\n                        pose_directory = os.path.join(working_path, word,'pose_keypoints')\n                        lh_directory = os.path.join(working_path, word,'lh_keypoints')\n                        rh_directory = os.path.join(working_path, word,'rh_keypoints')\n\n                        if not os.path.exists(pose_directory):\n                            os.makedirs(pose_directory)\n\n                        if not os.path.exists(lh_directory):\n                            os.makedirs(lh_directory)\n\n                        if not os.path.exists(rh_directory):\n                            os.makedirs(rh_directory)\n\n                        pose_path = os.path.join(pose_directory, video_file)\n                        np.save(pose_path, pose_keypoints)\n\n                        lh_path = os.path.join(lh_directory, video_file)\n                        np.save(lh_path, lh_keypoints)\n\n                        rh_path = os.path.join(rh_directory, video_file)\n                        np.save(rh_path, rh_keypoints)","metadata":{"execution":{"iopub.status.busy":"2023-11-06T22:15:48.144799Z","iopub.execute_input":"2023-11-06T22:15:48.145368Z","iopub.status.idle":"2023-11-06T22:15:48.174619Z","shell.execute_reply.started":"2023-11-06T22:15:48.145297Z","shell.execute_reply":"2023-11-06T22:15:48.172822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extract_keypoints(results):\n\n    pose = np.array([[res.x, res.y, res.z] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*3)\n    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n    nose=pose[:3]\n    lh_wrist=lh[:3]\n    rh_wrist=rh[:3]\n    pose_adjusted = adjust_landmarks(pose,nose)\n    lh_adjusted = adjust_landmarks(lh,lh_wrist)\n    rh_adjusted = adjust_landmarks(rh,rh_wrist)\n    return pose_adjusted, lh_adjusted, rh_adjusted","metadata":{"execution":{"iopub.status.busy":"2023-11-05T07:13:32.982763Z","iopub.execute_input":"2023-11-05T07:13:32.983172Z","iopub.status.idle":"2023-11-05T07:13:32.991858Z","shell.execute_reply.started":"2023-11-05T07:13:32.983137Z","shell.execute_reply":"2023-11-05T07:13:32.990790Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def adjust_landmarks(arr,center):\n\n    # Reshape the array to have shape (n, 3)\n    arr_reshaped = arr.reshape(-1, 3)\n\n    # Repeat the center array to have shape (n, 3)\n    center_repeated = np.tile(center, (len(arr_reshaped), 1))\n\n    # Subtract the center array from the arr array\n    arr_adjusted = arr_reshaped - center_repeated\n\n    # Reshape arr_adjusted back to shape (n*3,)\n    arr_adjusted = arr_adjusted.reshape(-1)\n    return(arr_adjusted)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-05T07:13:34.103638Z","iopub.execute_input":"2023-11-05T07:13:34.104014Z","iopub.status.idle":"2023-11-05T07:13:34.109430Z","shell.execute_reply.started":"2023-11-05T07:13:34.103984Z","shell.execute_reply":"2023-11-05T07:13:34.108279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def draw_styled_landmarks(image, results):\n\n    # Draw pose connections\n    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n                             )\n    # Draw left hand connections\n    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n                             )\n    # Draw right hand connections\n    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n                             )","metadata":{"execution":{"iopub.status.busy":"2023-11-05T07:13:35.639791Z","iopub.execute_input":"2023-11-05T07:13:35.640601Z","iopub.status.idle":"2023-11-05T07:13:35.649244Z","shell.execute_reply.started":"2023-11-05T07:13:35.640566Z","shell.execute_reply":"2023-11-05T07:13:35.648268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_dir = '/kaggle/working/Dataset/frames'\n# Create necessary directories\nos.makedirs(dataset_dir, exist_ok=True)\nos.makedirs(os.path.join(dataset_dir, 'Train'), exist_ok=True)\nos.makedirs(os.path.join(dataset_dir, 'Test'), exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-05T07:24:41.763571Z","iopub.execute_input":"2023-11-05T07:24:41.765771Z","iopub.status.idle":"2023-11-05T07:24:41.771605Z","shell.execute_reply.started":"2023-11-05T07:24:41.765724Z","shell.execute_reply":"2023-11-05T07:24:41.770799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil \nshutil.copytree('/kaggle/input/greekkplastnew/npy_arrays','/kaggle/working/Dataset/npy_arrays')","metadata":{"execution":{"iopub.status.busy":"2023-11-06T21:03:22.481754Z","iopub.execute_input":"2023-11-06T21:03:22.482266Z","iopub.status.idle":"2023-11-06T21:03:31.097428Z","shell.execute_reply.started":"2023-11-06T21:03:22.482228Z","shell.execute_reply":"2023-11-06T21:03:31.096429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"selected_words = os.listdir('/kaggle/input/greekkplastnew/frames/Train')","metadata":{"execution":{"iopub.status.busy":"2023-11-06T21:03:50.885675Z","iopub.execute_input":"2023-11-06T21:03:50.887244Z","iopub.status.idle":"2023-11-06T21:03:50.895587Z","shell.execute_reply.started":"2023-11-06T21:03:50.887189Z","shell.execute_reply":"2023-11-06T21:03:50.894177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"make_keypoint_arrays('/kaggle/input/greekkplastnew/frames','Train/')\nmake_keypoint_arrays('/kaggle/input/greekkplastnew/frames','Test/')\nwords= np.array(os.listdir('/kaggle/input/greekkplastnew/frames/Train'))\nprint(words)","metadata":{"execution":{"iopub.status.busy":"2023-11-06T22:47:40.215686Z","iopub.execute_input":"2023-11-06T22:47:40.216204Z","iopub.status.idle":"2023-11-06T22:47:40.225633Z","shell.execute_reply.started":"2023-11-06T22:47:40.216165Z","shell.execute_reply":"2023-11-06T22:47:40.224394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# words = selected_words\nlabel_map = {label:num for num, label in enumerate(words)}\nprint(label_map)","metadata":{"execution":{"iopub.status.busy":"2023-11-06T22:48:37.914795Z","iopub.execute_input":"2023-11-06T22:48:37.915300Z","iopub.status.idle":"2023-11-06T22:48:37.923695Z","shell.execute_reply.started":"2023-11-06T22:48:37.915264Z","shell.execute_reply":"2023-11-06T22:48:37.922152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport keras\nimport numpy as np\nimport pandas as pd\nimport os\nimport shutil\nimport datetime as dt\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nimport glob\nimport cv2\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Bidirectional, LSTM, Dense\nfrom keras.callbacks import EarlyStopping\nfrom sklearn.metrics import multilabel_confusion_matrix, accuracy_score\nfrom sklearn.model_selection import train_test_split\nnp.random.seed(42)","metadata":{"execution":{"iopub.status.busy":"2023-11-06T22:49:38.002985Z","iopub.execute_input":"2023-11-06T22:49:38.003420Z","iopub.status.idle":"2023-11-06T22:49:38.013952Z","shell.execute_reply.started":"2023-11-06T22:49:38.003389Z","shell.execute_reply":"2023-11-06T22:49:38.011885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_data(data_path,split,f_avg):\n\n    # Initialize the lists of sequences and labels\n    sequences = []\n    labels = []\n\n    # Iterate over the words\n    for word in tqdm(words):\n        word_path = os.path.join(data_path, split, word)\n        word_id = os.path.basename(word_path).zfill(4)\n\n        lh_keypoints_folder = os.path.join(word_path, \"lh_keypoints\")\n        rh_keypoints_folder = os.path.join(word_path, \"rh_keypoints\")\n        pose_keypoints_folder = os.path.join(word_path, \"pose_keypoints\")\n\n        # Iterate through the sequences (numpy arrays) contained in the lh_keypoints folder\n        for sequence in os.listdir(lh_keypoints_folder):\n            # Load the left hand array\n            res_lh = np.load(os.path.join(lh_keypoints_folder, sequence))\n\n            # Determine how many frames to select\n            num_frames = min(res_lh.shape[0], f_avg)\n            res_lh = res_lh[:num_frames, :]\n            while num_frames < f_avg:\n                res_lh = np.concatenate((res_lh, np.expand_dims(res_lh[-1, :], axis=0)), axis=0)\n                num_frames += 1\n\n            # Load the right hand array\n            res_rh = np.load(os.path.join(rh_keypoints_folder, sequence))\n\n            # Determine how many frames to select\n            num_frames = min(res_rh.shape[0], f_avg)\n            res_rh = res_rh[:num_frames, :]\n            while num_frames < f_avg:\n                res_rh = np.concatenate((res_rh, np.expand_dims(res_rh[-1, :], axis=0)), axis=0)\n                num_frames += 1\n\n            # Load the pose array\n            res_pose = np.load(os.path.join(pose_keypoints_folder, sequence))\n\n            # Determine how many frames to select\n            num_frames = min(res_pose.shape[0], f_avg)\n            res_pose = res_pose[:num_frames, :]\n            while num_frames < f_avg:\n                res_pose = np.concatenate((res_pose, np.expand_dims(res_pose[-1, :], axis=0)), axis=0)\n                num_frames += 1\n\n            # Append the subsequence to the list of sequences\n            sequences.append(np.concatenate((res_pose, res_lh, res_rh), axis=1))\n            # Append the label to the list of labels\n            labels.append(label_map[word])\n\n    # Convert the lists of sequences and labels to numpy arrays\n    X = np.array(sequences)\n    y = to_categorical(labels).astype(int)\n\n    return X, y","metadata":{"execution":{"iopub.status.busy":"2023-11-06T22:49:51.222141Z","iopub.execute_input":"2023-11-06T22:49:51.223219Z","iopub.status.idle":"2023-11-06T22:49:51.240549Z","shell.execute_reply.started":"2023-11-06T22:49:51.223179Z","shell.execute_reply":"2023-11-06T22:49:51.238838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = '/kaggle/working/Dataset/npy_arrays/Test'\nfor word in os .listdir(path):\n    cpath = os.path.join(path,word)\n    for cat in os.listdir(cpath):\n        vpath = os.path.join(cpath,cat)\n        for vid in os.listdir(vpath):\n            if '(1)(1)' in vid:\n                os.remove(os.path.join(vpath,vid))","metadata":{"execution":{"iopub.status.busy":"2023-11-06T22:57:04.787359Z","iopub.execute_input":"2023-11-06T22:57:04.788118Z","iopub.status.idle":"2023-11-06T22:57:04.835406Z","shell.execute_reply.started":"2023-11-06T22:57:04.788079Z","shell.execute_reply":"2023-11-06T22:57:04.833593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train and validation splits\ndata_path = '/kaggle/working/Dataset/npy_arrays'\n\nX_train,y_train=preprocess_data(data_path,'Train/',48)\n# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\nprint(X_train.shape)\nprint(y_train.shape)\n# print(X_val.shape)\n# print(y_val.shape)","metadata":{"execution":{"iopub.status.busy":"2023-11-06T22:58:34.774485Z","iopub.execute_input":"2023-11-06T22:58:34.774989Z","iopub.status.idle":"2023-11-06T22:58:37.655652Z","shell.execute_reply.started":"2023-11-06T22:58:34.774953Z","shell.execute_reply":"2023-11-06T22:58:37.654274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#test split\nX_test,y_test=preprocess_data(data_path,'Test',48)\nprint(X_test.shape)\nprint(y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2023-11-06T22:58:47.525350Z","iopub.execute_input":"2023-11-06T22:58:47.525725Z","iopub.status.idle":"2023-11-06T22:58:48.182323Z","shell.execute_reply.started":"2023-11-06T22:58:47.525697Z","shell.execute_reply":"2023-11-06T22:58:48.181201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = tf.keras.Sequential([\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16)),\n#     tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(len(words), activation='softmax')\n])\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n# Set up early stopping\nearly_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss',  # Metric to monitor for early stopping\n    mode='min',  # Set mode to 'min' for minimizing the metric\n    patience=30,  # Number of epochs with no improvement before stopping\n    restore_best_weights=True,  # Restore the best model weights\n    verbose=1\n)\n# Define the schedule function\ndef scheduler(epoch, lr):\n    if epoch < 10:\n        return lr\n    else:\n        return lr * 0.9 ** (epoch // 10000)\n\n# Create the LearningRateScheduler callback\ncallback = tf.keras.callbacks.LearningRateScheduler(scheduler)\nes=tf.keras.callbacks.EarlyStopping( monitor=\"val_loss\", patience=1,\n                                     verbose=1,  restore_best_weights=True)\nrlronp=tf.keras.callbacks.ReduceLROnPlateau( monitor=\"val_loss\", factor=0.5, patience=3,\n                                             verbose=1,mode = 'min')\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-06T22:59:30.734064Z","iopub.execute_input":"2023-11-06T22:59:30.734514Z","iopub.status.idle":"2023-11-06T22:59:30.973969Z","shell.execute_reply.started":"2023-11-06T22:59:30.734480Z","shell.execute_reply":"2023-11-06T22:59:30.972635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_training_history = model.fit(X_train, y_train,batch_size=4,validation_batch_size=4, validation_data=(X_test, y_test), epochs=200,callbacks=[early_stopping,rlronp,callback])","metadata":{"execution":{"iopub.status.busy":"2023-11-06T22:59:42.096348Z","iopub.execute_input":"2023-11-06T22:59:42.096867Z","iopub.status.idle":"2023-11-06T23:37:04.718011Z","shell.execute_reply.started":"2023-11-06T22:59:42.096830Z","shell.execute_reply":"2023-11-06T23:37:04.716687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate the model on train data\nmodel_evaluation_history = model.evaluate(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-11-06T23:37:04.721062Z","iopub.execute_input":"2023-11-06T23:37:04.723133Z","iopub.status.idle":"2023-11-06T23:37:09.233263Z","shell.execute_reply.started":"2023-11-06T23:37:04.723075Z","shell.execute_reply":"2023-11-06T23:37:09.232280Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate the model on test data\nmodel_evaluation_history = model.evaluate(X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2023-11-06T23:37:09.234976Z","iopub.execute_input":"2023-11-06T23:37:09.236165Z","iopub.status.idle":"2023-11-06T23:37:09.628947Z","shell.execute_reply.started":"2023-11-06T23:37:09.236131Z","shell.execute_reply":"2023-11-06T23:37:09.627809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_metric(model_training_history, metric_name_1, metric_name_2, plot_name):\n\n\n    # Get metric values using metric names as identifiers.\n    metric_value_1 = model_training_history.history[metric_name_1]\n    metric_value_2 = model_training_history.history[metric_name_2]\n\n    # Construct a range object which will be used as x-axis (horizontal plane) of the graph.\n    epochs = range(len(metric_value_1))\n\n    # Plot the Graph.\n    plt.plot(epochs, metric_value_1, 'blue', label = metric_name_1)\n    plt.plot(epochs, metric_value_2, 'red', label = metric_name_2)\n\n    # Add title to the plot.\n    plt.title(str(plot_name))\n\n    # Add legend to the plot.\n    plt.legend()","metadata":{"execution":{"iopub.status.busy":"2023-11-06T23:37:19.018078Z","iopub.execute_input":"2023-11-06T23:37:19.019708Z","iopub.status.idle":"2023-11-06T23:37:19.028256Z","shell.execute_reply.started":"2023-11-06T23:37:19.019662Z","shell.execute_reply":"2023-11-06T23:37:19.026721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize the training and validation loss metrices.\nplot_metric(model_training_history, 'loss', 'val_loss', 'Total Loss vs Total Validation Loss')","metadata":{"execution":{"iopub.status.busy":"2023-11-06T23:37:30.285555Z","iopub.execute_input":"2023-11-06T23:37:30.286208Z","iopub.status.idle":"2023-11-06T23:37:30.717899Z","shell.execute_reply.started":"2023-11-06T23:37:30.286161Z","shell.execute_reply":"2023-11-06T23:37:30.716535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize the training and validation accuracy metrices.\nplot_metric(model_training_history, 'categorical_accuracy', 'val_categorical_accuracy', 'Total Accuracy vs Total Validation Accuracy')","metadata":{"execution":{"iopub.status.busy":"2023-11-06T23:37:44.047065Z","iopub.execute_input":"2023-11-06T23:37:44.047728Z","iopub.status.idle":"2023-11-06T23:37:44.415197Z","shell.execute_reply.started":"2023-11-06T23:37:44.047694Z","shell.execute_reply":"2023-11-06T23:37:44.414174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Predicted sign\nres = model.predict(X_test)\nwords[np.argmax(res[1])]","metadata":{"execution":{"iopub.status.busy":"2023-11-06T23:39:25.756032Z","iopub.execute_input":"2023-11-06T23:39:25.756521Z","iopub.status.idle":"2023-11-06T23:39:28.066823Z","shell.execute_reply.started":"2023-11-06T23:39:25.756485Z","shell.execute_reply":"2023-11-06T23:39:28.065249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Real sign\nwords[np.argmax(y_test[1])]\n","metadata":{"execution":{"iopub.status.busy":"2023-11-06T23:39:49.016411Z","iopub.execute_input":"2023-11-06T23:39:49.017548Z","iopub.status.idle":"2023-11-06T23:39:49.025147Z","shell.execute_reply.started":"2023-11-06T23:39:49.017490Z","shell.execute_reply":"2023-11-06T23:39:49.024271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the loss and accuracy from model_evaluation_history.\nmodel_evaluation_loss, model_evaluation_accuracy = model_evaluation_history","metadata":{"execution":{"iopub.status.busy":"2023-11-06T23:40:03.417692Z","iopub.execute_input":"2023-11-06T23:40:03.418146Z","iopub.status.idle":"2023-11-06T23:40:03.423558Z","shell.execute_reply.started":"2023-11-06T23:40:03.418113Z","shell.execute_reply":"2023-11-06T23:40:03.422273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the string date format.\n# Get the current Date and Time in a DateTime Object.\n# Convert the DateTime object to string according to the style mentioned in date_time_format string.\ndate_time_format = '%Y_%m_%d__%H_%M_%S'\ncurrent_date_time_dt = dt.datetime.now()\ncurrent_date_time_string = dt.datetime.strftime(current_date_time_dt, date_time_format)\n\n# Define a useful name for our model to make it easy for us while navigating through multiple saved models.\nmodel_file_name = f'VSL_Date_Time_{current_date_time_string}___Loss_{model_evaluation_loss}___Accuracy_{model_evaluation_accuracy}.h5'\n\nos.makedirs('/kaggle/working/Model',exist_ok=True)\n# Save your Model.\nmodelpath = f'/kaggle/working/Model/{model_file_name}'\nmodel.save(modelpath)","metadata":{"execution":{"iopub.status.busy":"2023-11-06T23:40:37.185569Z","iopub.execute_input":"2023-11-06T23:40:37.186980Z","iopub.status.idle":"2023-11-06T23:40:37.257932Z","shell.execute_reply.started":"2023-11-06T23:40:37.186914Z","shell.execute_reply":"2023-11-06T23:40:37.256923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yhat = model.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-11-06T23:40:50.820338Z","iopub.execute_input":"2023-11-06T23:40:50.820883Z","iopub.status.idle":"2023-11-06T23:40:51.172878Z","shell.execute_reply.started":"2023-11-06T23:40:50.820843Z","shell.execute_reply":"2023-11-06T23:40:51.171402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_key_by_value(dictionary, value):\n    for key, val in dictionary.items():\n        if val == value:\n            return key\n    return None\nytrue = np.argmax(y_test, axis=1).tolist()\nyhat = np.argmax(yhat, axis=1).tolist()\n\ny = []\nfor v in ytrue:\n    y.append(get_key_by_value(label_map, v))\nprint(y)","metadata":{"execution":{"iopub.status.busy":"2023-11-06T23:41:06.287795Z","iopub.execute_input":"2023-11-06T23:41:06.288324Z","iopub.status.idle":"2023-11-06T23:41:06.302738Z","shell.execute_reply.started":"2023-11-06T23:41:06.288268Z","shell.execute_reply":"2023-11-06T23:41:06.301253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ypred = []\nfor v in yhat:\n    ypred.append(get_key_by_value(label_map, v))\nprint(ypred)","metadata":{"execution":{"iopub.status.busy":"2023-11-06T23:41:23.127935Z","iopub.execute_input":"2023-11-06T23:41:23.128688Z","iopub.status.idle":"2023-11-06T23:41:23.138742Z","shell.execute_reply.started":"2023-11-06T23:41:23.128620Z","shell.execute_reply":"2023-11-06T23:41:23.137271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# Assuming y and ypred are your target labels and predicted labels, respectively\n\ny_subset = y\nypred_subset = ypred\n\n# Get unique class labels\nclass_labels = np.unique(y_subset)\n\n# Compute confusion matrix\ncm = confusion_matrix(y_subset, ypred_subset, labels=class_labels)\n# Create a DataFrame from the confusion matrix\ndf_cm = pd.DataFrame(cm, index=class_labels, columns=class_labels)\ndf_cm.index.name = 'Actual'\ndf_cm.columns.name = 'Predicted'\n# Plot the confusion matrix\nplt.figure(figsize=(len(words), len(words)*0.8))\nsns.set(font_scale=1.3)  # for label size\nsns.heatmap(df_cm, cmap=\"Blues\", annot=True, fmt=\"d\", annot_kws={\"size\": 12})\nplt.title(\"Confusion Matrix\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-06T23:42:21.445161Z","iopub.execute_input":"2023-11-06T23:42:21.445768Z","iopub.status.idle":"2023-11-06T23:42:50.435569Z","shell.execute_reply.started":"2023-11-06T23:42:21.445729Z","shell.execute_reply":"2023-11-06T23:42:50.433446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import the libraries\nimport tensorflow as tf\nfrom tensorflow import keras\n\n# Load the H5 model\nmodel = keras.models.load_model(modelpath)\n\n# Convert the model to TensorFlow Lite\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\nconverter._experimental_lower_tensor_list_ops = False\ntflite_model = converter.convert()\n\n# Save the TensorFlow Lite model\nwith open(\"/kaggle/working/GreekModel.tflite\", \"wb\") as f:\n  f.write(tflite_model)","metadata":{"execution":{"iopub.status.busy":"2023-11-06T23:43:55.083599Z","iopub.execute_input":"2023-11-06T23:43:55.084074Z","iopub.status.idle":"2023-11-06T23:45:06.522080Z","shell.execute_reply.started":"2023-11-06T23:43:55.084037Z","shell.execute_reply":"2023-11-06T23:45:06.520889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}