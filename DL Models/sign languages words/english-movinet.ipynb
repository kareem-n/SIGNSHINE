{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2632847,"sourceType":"datasetVersion","datasetId":1589971},{"sourceId":6618583,"sourceType":"datasetVersion","datasetId":3819843},{"sourceId":148503680,"sourceType":"kernelVersion"}],"dockerImageVersionId":30558,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testFile = pd.read_csv('/kaggle/input/englishdatasetbig/ASL_Citizen/splits/test.csv')\ntestFile.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valFile = pd.read_csv('/kaggle/input/englishdatasetbig/ASL_Citizen/splits/val.csv')\nvalFile.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainFile = pd.read_csv('/kaggle/input/englishdatasetbig/ASL_Citizen/splits/train.csv')\ntrainFile.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"words = []\nmapOfTrain = {}\n# mapOfTest ={}\nmapOfVal = {}\nfor word in trainFile['Gloss'].values:\n    if len(mapOfTrain)<100:\n        words.append(word)\n        mapOfTrain[word]=[]\n        mapOfVal[word] =[]\n        for i in trainFile[trainFile['Gloss']==word]['Video file'].values:\n            if len(mapOfTrain[word])<25:\n                mapOfTrain[word].append(i)\n        for i in (testFile[testFile['Gloss']==word]['Video file'].values):\n            if len(mapOfTrain[word])<25:\n                mapOfTrain[word].append(i)\n        for i in valFile[valFile['Gloss']==word]['Video file'].values:\n            mapOfVal[word].append(i)\n    else:\n        break\nfor k,v in mapOfTrain.items():\n    if len(v) <25:\n        print(k,'f'*10)\n    print('tr',k,len(v))\n#     print('ts',k,len(mapOfTest[k]))\n    print('val',k,len(mapOfVal[k]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# newVidsMap = {}\n# for word in words:\n#     newVidsMap[word] = vidsMap[word]\n# len(newVidsMap),newVidsMap[words[0]][0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install remotezip tqdm opencv-python opencv-python-headless tf-models-official\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install mediapy\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tqdm\nimport random\nimport pathlib\nimport itertools\nimport collections\nfrom pathlib import Path\n%matplotlib inline\nimport os\nimport cv2\nimport numpy as np\nimport remotezip as rz\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport seaborn as sns\n# Some modules to display an animation using imageio.\nimport imageio\nfrom IPython import display\nfrom urllib import request\n#from tensorflow_docs.vis import embed\nimport keras\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.optimizers.legacy import Adam\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\nimport mediapy as media\n\n\n# Import the MoViNet model from TensorFlow Models (tf-models-official) for the MoViNet model\nfrom official.projects.movinet.modeling import movinet\nfrom official.projects.movinet.modeling import movinet_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Num classes:', len(mapOfTrain))\nprint('Num videos for class[0]:', len(mapOfTrain[words[-1]]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NUM_CLASSES = 100\nFILES_PER_CLASS = 25","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def select_subset_of_classes(files_for_class, classes, files_per_class):\n  \"\"\" Create a dictionary with the class name and a subset of the files in that class.\n\n    Args:\n      files_for_class: Dictionary of class names (key) and files (values).\n      classes: List of classes.\n      files_per_class: Number of files per class of interest.\n\n    Returns:\n      Dictionary with class as key and list of specified number of video files in that class.\n  \"\"\"\n  files_subset = dict()\n\n  for class_name in classes:\n    class_files = files_for_class[class_name]\n    files_subset[class_name] = class_files[:files_per_class]\n\n  return files_subset\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"files_subset = select_subset_of_classes(mapOfTrain, words[:NUM_CLASSES], FILES_PER_CLASS)\nfiles_subset_test = select_subset_of_classes(mapOfVal, words[:NUM_CLASSES], 3)\n\nlist(files_subset.keys())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(files_subset[words[0]])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def split_class_lists(files_for_class, count):\n  \"\"\" Returns the list of files belonging to a subset of data as well as the remainder of\n    files that need to be downloaded.\n\n    Args:\n      files_for_class: Files belonging to a particular class of data.\n      count: Number of files to download.\n\n    Returns:\n      Files belonging to the subset of data and dictionary of the remainder of files that need to be downloaded.\n  \"\"\"\n  split_files = []\n  remainder = {}\n  for cls in files_for_class.keys():\n    split_files.extend(files_for_class[cls][:count])\n    remainder[cls] = files_for_class[cls][count:]\n  return split_files, remainder","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport shutil\nimport tqdm\nimport pathlib\n\ndef read_from_dir(dir_path, to_dir, file_names):\n  \"\"\" Read the contents of the files from the directory path.\n\n    Args:\n      dir_path: A path to a local directory containing data.\n      to_dir: A directory to copy data to.\n      file_names: Names of files to copy.\n  \"\"\"\n  for fn in tqdm.tqdm(file_names):\n    class_name = fn.split('/')[-1]\n    file_path = fn.split(f'/{class_name}')[0]\n    output_dir = os.path.join(to_dir, class_name)\n    os.makedirs(output_dir, exist_ok=True)\n    try:\n        shutil.copy(file_path, output_dir)\n    except Exception:\n        print(file_path,output_dir)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"files_subset_new ={}\nfor k,v in files_subset.items():\n    files_subset_new[k]=[]\n    for path in v:\n        files_subset_new[k].append(f'/kaggle/input/englishdatasetbig/ASL_Citizen/videos/{path}/{k}')\nfiles_subset_new[k][0].split(f'/{files_subset_new[k][0].split(\"/\")[-1]}')[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"files_subset_new_test ={}\nfor k,v in files_subset_test.items():\n    files_subset_new_test[k]=[]\n    for path in v:\n        files_subset_new_test[k].append(f'/kaggle/input/englishdatasetbig/ASL_Citizen/videos/{path}/{k}')\nfiles_subset_new_test[k][0].split(f'/{files_subset_new[k][0].split(\"/\")[-1]}')[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def download_ucf_101_subset(zip_url, num_classes, splits, download_dir):\n    for k,v in files_subset_new.items():\n        for f in v:\n            path = os.path.normpath(f.split(f'/{k}')[0])\n            tokens = path.split(os.sep)\n\n#     files_for_class = get_files_per_class(files)\n\n    classes = files_subset_new.keys()\n\n    for cls in classes:\n        random.shuffle(files_subset_new[cls])\n\n#     # Only use the number of classes you want in the dictionary\n#     files_for_class = {x: files_for_class[x] for x in classes}\n\n    dirs = {}\n    for split_name, split_count in splits.items():\n        print(split_name, \":\")\n        split_dir = Path('/kaggle/working/output') / split_name\n\n        split_files, files_for_class = split_class_lists(files_subset_new, split_count)\n        read_from_dir(zip_url, split_dir, split_files)\n        dirs[split_name] = split_dir\n\n    return dirs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def download_ucf_101_subset_test(zip_url, num_classes, splits, download_dir):\n    for k,v in files_subset_new_test.items():\n        for f in v:\n            path = os.path.normpath(f.split(f'/{k}')[0])\n            tokens = path.split(os.sep)\n\n#     files_for_class = get_files_per_class(files)\n\n    classes = files_subset_new_test.keys()\n\n    for cls in classes:\n        random.shuffle(files_subset_new_test[cls])\n\n#     # Only use the number of classes you want in the dictionary\n#     files_for_class = {x: files_for_class[x] for x in classes}\n\n    dirs = {}\n    for split_name, split_count in splits.items():\n        print(split_name, \":\")\n        split_dir = Path('/kaggle/working/output') / split_name\n\n        split_files, files_for_class = split_class_lists(files_subset_new_test, split_count)\n        read_from_dir(zip_url, split_dir, split_files)\n        dirs[split_name] = split_dir\n\n    return dirs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# shutil.rmtree('/kaggle/working/output')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"download_dir = pathlib.Path('/kaggle/input/englishdatasetbig/ASL_Citizen/videos')\noutput = '/kaggle/working/output'\nsubset_paths = download_ucf_101_subset(output,\n                                       num_classes = NUM_CLASSES,\n                                       splits = {\"train\": 10, \"test\": 10},\n                                       download_dir = download_dir)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"download_dir_test = pathlib.Path('/kaggle/input/englishdatasetbig/ASL_Citizen/videos')\noutput = '/kaggle/working/output'\nsubset_paths ['val'] = ( download_ucf_101_subset_test(output,\n                                       num_classes = NUM_CLASSES,\n                                       splits = {\"val\": 5},\n                                       download_dir = download_dir_test))['val']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import shutil\n# shutil.rmtree('/kaggle/working/output')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output = pathlib.Path('/kaggle/working/output')\nvideo_count_train = len(list(output.glob('train/*/*.mp4')))\nvideo_count_val = len(list(output.glob('val/*/*.mp4')))\nvideo_count_test = len(list(output.glob('test/*/*.mp4')))\nprint(video_count_train)\nprint(video_count_val)\n\nprint(video_count_test)\n\nvideo_total = video_count_train + video_count_test+video_count_val\nprint(f\"Total videos: {video_total}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"subset_paths\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def format_frames(frame, output_size):\n  \"\"\"\n    Pad and resize an image from a video.\n    \n    Args:\n      frame: Image that needs to resized and padded. \n      output_size: Pixel size of the output frame image.\n\n    Return:\n      Formatted frame with padding of specified output size.\n  \"\"\"\n  frame = tf.image.convert_image_dtype(frame, tf.float32)\n  frame = tf.image.resize_with_pad(frame, *output_size)\n  return frame","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def frames_from_video_file(video_path, n_frames, output_size = (224,224), frame_step = 10):\n  \"\"\"\n    Creates frames from each video file present for each category.\n\n    Args:\n      video_path: File path to the video.\n      n_frames: Number of frames to be created per video file.\n      output_size: Pixel size of the output frame image.\n\n    Return:\n      An NumPy array of frames in the shape of (n_frames, height, width, channels).\n  \"\"\"\n  # Read each video frame by frame\n  result = []\n  src = cv2.VideoCapture(str(video_path))  \n\n  video_length = src.get(cv2.CAP_PROP_FRAME_COUNT)\n\n  need_length = 1 + (n_frames - 1) * frame_step\n\n  if need_length > video_length:\n    start = 0\n  else:\n    max_start = video_length - need_length\n    start = random.randint(0, max_start + 1)\n\n  src.set(cv2.CAP_PROP_POS_FRAMES, start)\n  # ret is a boolean indicating whether read was successful, frame is the image itself\n  ret, frame = src.read()\n  result.append(format_frames(frame, output_size))\n\n  for _ in range(n_frames - 1):\n    for _ in range(frame_step):\n      ret, frame = src.read()\n    if ret:\n      frame = format_frames(frame, output_size)\n      result.append(frame)\n    else:\n      result.append(np.zeros_like(result[0]))\n  src.release()\n  result = np.array(result)[..., [2, 1, 0]]\n\n  return result\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FrameGenerator:\n  def __init__(self, path, n_frames, training = False):\n    \"\"\" Returns a set of frames with their associated label. \n\n      Args:\n        path: Video file paths.\n        n_frames: Number of frames. \n        training: Boolean to determine if training dataset is being created.\n    \"\"\"\n    self.path = path\n    self.n_frames = n_frames\n    self.training = training\n    self.class_names = sorted(set(p.name for p in self.path.iterdir() if p.is_dir()))\n    self.class_ids_for_name = dict((name, idx) for idx, name in enumerate(self.class_names))\n    \n  def get_files_and_class_names(self):\n    video_paths = list(self.path.glob('*/*.mp4'))\n    classes = [p.parent.name for p in video_paths] \n    return video_paths, classes\n\n  def __call__(self):\n    video_paths, classes = self.get_files_and_class_names()\n\n    pairs = list(zip(video_paths, classes))\n\n    if self.training:\n      random.shuffle(pairs)\n    labelmap = {}\n    for path, name in pairs:\n      video_frames = frames_from_video_file(path, self.n_frames) \n      label = self.class_ids_for_name[name] # Encode labels\n      yield video_frames, label\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 8\nnum_frames = 8\n\noutput_signature = (tf.TensorSpec(shape = (None, None, None, 3), dtype = tf.float32),\n                    tf.TensorSpec(shape = (), dtype = tf.int16))\n\ntrain_ds = tf.data.Dataset.from_generator(FrameGenerator(subset_paths['train'], num_frames, training = True),\n                                          output_signature = output_signature)\ntrain_labels = FrameGenerator(subset_paths['train'], num_frames, training = True).class_names\ntrain_ds = train_ds.batch(batch_size)\n\ntest_ds = tf.data.Dataset.from_generator(FrameGenerator(subset_paths['test'], num_frames),\n                                         output_signature = output_signature)\ntest_labels = FrameGenerator(subset_paths['test'], num_frames).class_names\ntest_ds = test_ds.batch(batch_size)\n\nval_ds = tf.data.Dataset.from_generator(FrameGenerator(subset_paths['val'], num_frames),\n                                          output_signature = output_signature)\nval_labels = FrameGenerator(subset_paths['val'], num_frames).class_names\nval_ds = val_ds.batch(batch_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for frames, labels in train_ds.take(4):\n  print(labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Shape: {frames.shape}\")\nprint(f\"Label: {labels.shape}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"videos, labels = next(iter(train_ds))\nmedia.show_videos(videos.numpy(), codec='gif', fps=8)\ntrain_labels[labels[0]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"videos, labels = next(iter(val_ds))\nmedia.show_videos(videos.numpy(), codec='gif', fps=8)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_id = 'a0'\nresolution = 224\n\ntf.keras.backend.clear_session()\n\nbackbone = movinet.Movinet(model_id=model_id)\nbackbone.trainable = False\n\n# Set num_classes=600 to load the pre-trained weights from the original model\nmodel = movinet_model.MovinetClassifier(backbone=backbone, num_classes=600)\nmodel.build([None, None, None, None, 3])\n\n# Load pre-trained weights\n!wget https://storage.googleapis.com/tf_model_garden/vision/movinet/movinet_a0_base.tar.gz -O movinet_a0_base.tar.gz -q\n!tar -xvf movinet_a0_base.tar.gz\n\ncheckpoint_dir = f'movinet_{model_id}_base'\ncheckpoint_path = tf.train.latest_checkpoint(checkpoint_dir)\ncheckpoint = tf.train.Checkpoint(model=model)\nstatus = checkpoint.restore(checkpoint_path)\nstatus.assert_existing_objects_matched()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_classifier(batch_size, num_frames, resolution, backbone, num_classes):\n  \"\"\"Builds a classifier on top of a backbone model.\"\"\"\n  model = movinet_model.MovinetClassifier(\n      backbone=backbone,\n      num_classes=num_classes)\n  model.build([batch_size, num_frames, resolution, resolution, 3])\n\n  return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size, num_frames, resolution, backbone, NUM_CLASSES","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = build_classifier(batch_size, num_frames, resolution, backbone, NUM_CLASSES)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 100\nloss_obj = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n\noptimizer = tf.keras.optimizers.Adam()\n\nmodel.compile(loss=loss_obj, optimizer=optimizer, metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds, test_ds,val_ds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"early_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss',  # Metric to monitor for early stopping\n    mode='min',  # Set mode to 'min' for minimizing the metric\n    patience=10,  # Number of epochs with no improvement before stopping\n    restore_best_weights=True,  # Restore the best model weights\n    verbose=1\n)\nrlronp=tf.keras.callbacks.ReduceLROnPlateau( monitor=\"val_loss\", factor=0.5, patience=1,\n                                             verbose=1,mode = 'min')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = model.fit(train_ds,\n                    validation_data=val_ds,\n                    epochs=100,\n                    validation_freq=1,\n                    verbose=1,batch_size=1024,callbacks=[early_stopping,rlronp])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the string date format.\n# Get the current Date and Time in a DateTime Object.\n# Convert the DateTime object to string according to the style mentioned in date_time_format string.\n\n# Define a useful name for our model to make it easy for us while navigating through multiple saved models.\nmodel_file_name = f'VSL_Date_Time'\nos.makedirs('/kaggle/working/Model',exist_ok=True)\n# Save your Model.\nmodelpath = f'/kaggle/working/Model/{model_file_name}'\nmodel.save(modelpath)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_evaluation_history = model.evaluate(test_ds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_evaluation_history = model.evaluate(train_ds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_evaluation_history = model.evaluate(val_ds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_metric(model_training_history, metric_name_1, metric_name_2, plot_name):\n\n\n    # Get metric values using metric names as identifiers.\n    metric_value_1 = model_training_history.history[metric_name_1]\n    metric_value_2 = model_training_history.history[metric_name_2]\n\n    # Construct a range object which will be used as x-axis (horizontal plane) of the graph.\n    epochs = range(len(metric_value_1))\n\n    # Plot the Graph.\n    plt.plot(epochs, metric_value_1, 'blue', label = metric_name_1)\n    plt.plot(epochs, metric_value_2, 'red', label = metric_name_2)\n\n    # Add title to the plot.\n    plt.title(str(plot_name))\n\n    # Add legend to the plot.\n    plt.legend()\n# Visualize the training and validation loss metrices.\nplot_metric(results, 'loss', 'val_loss', 'Total Loss vs Total Validation Loss')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize the training and validation accuracy metrices.\nplot_metric(results, 'accuracy', 'val_accuracy', 'Total Accuracy vs Total Validation Accuracy')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"k = train_ds.take(1)\nlabels = list(k.map(lambda x, y: y).unbatch().as_numpy_iterator())\nprint(labels[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}