{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c84114b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on all addresses.\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      " * Running on http://192.168.43.177:5000/ (Press CTRL+C to quit)\n",
      "192.168.43.1 - - [26/Dec/2023 02:25:07] \"POST /text_to_sign HTTP/1.1\" 200 -\n",
      "192.168.43.1 - - [26/Dec/2023 02:26:14] \"POST /text_to_sign HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask,request,jsonify\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np \n",
    "import os\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from flask import Flask, render_template, request\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from keras.models import load_model\n",
    "from tensorflow.keras.utils import img_to_array\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "#Arabic\n",
    "model_fileTrain = \"C:/Users/DELL/Downloads/model/ARSLTrainModel.tflite\"\n",
    "model_fileCafe = \"C:/Users/DELL/Downloads/model/ARSLCafeModel.tflite\"\n",
    "model_fileBank = \"C:/Users/DELL/Downloads/model/ARSLBankModel.tflite\"\n",
    "model_fileHospital = \"C:/Users/DELL/Downloads/model/ARSLHospitalModel.tflite\"\n",
    "\n",
    "model_fileGreek = \"C:/Users/DELL/Downloads/model/GreekModel.tflite\"\n",
    "model_fileTur = \"C:/Users/DELL/Downloads/model/TurkishModel.tflite\"\n",
    "model_fileIndian = \"C:/Users/DELL/Downloads/model/IndianVids.tflite\"\n",
    "model_file2 = \"C:/Users/DELL/Downloads/model/modelar.tflite\"\n",
    "model_file3 = \"C:/Users/DELL/Downloads/model/modelen.tflite\"\n",
    "modelTur = \"C:/Users/DELL/Downloads/model/model_tur.tflite\"\n",
    "\n",
    "#indian \n",
    "interpreterInd = tf.lite.Interpreter(model_path=model_fileIndian)\n",
    "interpreterInd.allocate_tensors()\n",
    "input_detailsInd = interpreterInd.get_input_details()\n",
    "output_detailsInd = interpreterInd.get_output_details()\n",
    "\n",
    "#ArabBank\n",
    "interpreterBank = tf.lite.Interpreter(model_path=model_fileBank)\n",
    "interpreterBank.allocate_tensors()\n",
    "input_detailsBank = interpreterBank.get_input_details()\n",
    "output_detailsBank = interpreterBank.get_output_details()\n",
    "#ArabCafe\n",
    "interpreterCafe = tf.lite.Interpreter(model_path=model_fileCafe)\n",
    "interpreterCafe.allocate_tensors()\n",
    "input_detailsCafe = interpreterCafe.get_input_details()\n",
    "output_detailsCafe = interpreterCafe.get_output_details()\n",
    "#ArabTrain\n",
    "interpreterTrain = tf.lite.Interpreter(model_path=model_fileTrain)\n",
    "interpreterTrain.allocate_tensors()\n",
    "input_detailsTrain = interpreterTrain.get_input_details()\n",
    "output_detailsTrain = interpreterTrain.get_output_details()\n",
    "#ArabHosp\n",
    "interpreterHospital = tf.lite.Interpreter(model_path=model_fileHospital)\n",
    "interpreterHospital.allocate_tensors()\n",
    "input_detailsHospital = interpreterHospital.get_input_details()\n",
    "output_detailsHospital = interpreterHospital.get_output_details()\n",
    "app = Flask(__name__)\n",
    "UPLOAD_FOLDER = 'videos'\n",
    "# Create the folder if it does not exist\n",
    "os.makedirs(UPLOAD_FOLDER, exist_ok=True)\n",
    "# Set the upload folder as an app configuration\n",
    "app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return \"Welcome To Our Server\"\n",
    "@app.route('/predictBank',methods=['POST'])\n",
    "def predictBank():\n",
    "    \n",
    "#     frames = request.body('Frames')\n",
    "    data = request.get_json()\n",
    "    frames = data['Frames']\n",
    "    framelist = []\n",
    "    counter = 0\n",
    "    # Loop over the list and process each bitmap object\n",
    "    for frame in frames:\n",
    "        image_value={}\n",
    "        for key, value in frame.items():\n",
    "            image_value = value\n",
    "        image_data = base64.b64decode(image_value['Image'])\n",
    "        img = Image.open(BytesIO(image_data))\n",
    "        image_path = f\"{app.config['UPLOAD_FOLDER']}{counter}.jpg\"\n",
    "        counter+=1\n",
    "        img.save(image_path, format=\"JPEG\")\n",
    "        framelist.append(f'{image_path}')\n",
    "#     counter = 0\n",
    "#     pathToFrames = []\n",
    "#     for frame in frames:\n",
    "#         filepath = os.path.join(app.config['UPLOAD_FOLDER'], f'{counter}.bmp')\n",
    "#         frame.save(filepath)\n",
    "#         pathToFrames.append(filepath)\n",
    "#         counter++\n",
    "#     framList = sorted(os.listdir(os.path.join(app.config['UPLOAD_FOLDER'])))\n",
    "    mp_holistic = mp.solutions.holistic\n",
    "    #_________________________________________________\n",
    "#     framList = []\n",
    "#     video_capture = cv2.VideoCapture(vid_file)\n",
    "#     ret, frame = video_capture.read()\n",
    "#     while ret:\n",
    "#         if frame is not None:\n",
    "#             frame = cv2.resize(frame, (224, 224), interpolation=cv2.INTER_AREA)\n",
    "#             framList.append(frame)\n",
    "#         ret, frame = video_capture.read()\n",
    "#     video_capture.release()\n",
    "    #________________________________________________\n",
    "    #*******************************************************************\n",
    "    pose_keypoints, lh_keypoints, rh_keypoints = [], [], []\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "      # Loop through the video frames\n",
    "      for frame in framelist:\n",
    "    #         image, results = mediapipe_detection(frame, holistic)\n",
    "            #///////////////////////////////////////////////////////\n",
    "            image = cv2.imread(frame)\n",
    "            if image is None:\n",
    "                raise ValueError(f\"Failed to load image: {image}\")\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "            image.flags.writeable = False                  # Image is no longer writeable\n",
    "            results = holistic.process(image)                 # Make prediction\n",
    "            image.flags.writeable = True                   # Image is now writeable\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "            #///////////////////////////////////////////////////////\n",
    "\n",
    "            # Extract keypoints\n",
    "    #         pose, lh, rh = extract_keypoints(results)\n",
    "            #>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "            pose = np.array([[res.x, res.y, res.z] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*3)\n",
    "            lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "            rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "            nose=pose[:3]\n",
    "            lh_wrist=lh[:3]\n",
    "            rh_wrist=rh[:3]\n",
    "    #         pose = adjust_landmarks(pose,nose)\n",
    "            ###########################################################\n",
    "            arr_reshaped = pose.reshape(-1, 3)\n",
    "            center_repeated = np.tile(nose, (len(arr_reshaped), 1))\n",
    "            arr_adjusted = arr_reshaped - center_repeated\n",
    "            pose = arr_adjusted.reshape(-1)\n",
    "            ###########################################################\n",
    "    #         lh = adjust_landmarks(lh,lh_wrist)\n",
    "            ###########################################################\n",
    "            arr_reshaped = lh.reshape(-1, 3)\n",
    "            center_repeated = np.tile(lh_wrist, (len(arr_reshaped), 1))\n",
    "            arr_adjusted = arr_reshaped - center_repeated\n",
    "            lh = arr_adjusted.reshape(-1)\n",
    "            ###########################################################\n",
    "    #         rh = adjust_landmarks(rh,rh_wrist)\n",
    "            ###########################################################\n",
    "            arr_reshaped = rh.reshape(-1, 3)\n",
    "            center_repeated = np.tile(rh_wrist, (len(arr_reshaped), 1))\n",
    "            arr_adjusted = arr_reshaped - center_repeated\n",
    "            rh = arr_adjusted.reshape(-1)\n",
    "            ###########################################################\n",
    "            #>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>.\n",
    "            # Add the keypoints to the list for this video\n",
    "            pose_keypoints.append(pose)\n",
    "            lh_keypoints.append(lh)\n",
    "            rh_keypoints.append(rh)     \n",
    "    #*********************************************************************\n",
    "    # Initialize the lists of sequences and labels\n",
    "    sequences = []\n",
    "    labels = []\n",
    "\n",
    "    # Iterate over the words\n",
    "\n",
    "    # Iterate through the sequences (numpy arrays) contained in the lh_keypoints folder\n",
    "    # Load the left hand array\n",
    "    res_lh = np.array(lh_keypoints)\n",
    "\n",
    "    # Determine how many frames to select\n",
    "    num_frames = min(res_lh.shape[0], 48)\n",
    "    res_lh = res_lh[:num_frames, :]\n",
    "    while num_frames < 48:\n",
    "        res_lh = np.concatenate((res_lh, np.expand_dims(res_lh[-1, :], axis=0)), axis=0)\n",
    "        num_frames += 1\n",
    "\n",
    "    # Load the right hand array\n",
    "    res_rh =  np.array(rh_keypoints)\n",
    "\n",
    "    # Determine how many frames to select\n",
    "    num_frames = min(res_rh.shape[0], 48)\n",
    "    res_rh = res_rh[:num_frames, :]\n",
    "    while num_frames < 48:\n",
    "        res_rh = np.concatenate((res_rh, np.expand_dims(res_rh[-1, :], axis=0)), axis=0)\n",
    "        num_frames += 1\n",
    "\n",
    "    # Load the pose array\n",
    "    res_pose =np.array(pose_keypoints) \n",
    "\n",
    "    # Determine how many frames to select\n",
    "    num_frames = min(res_pose.shape[0], 48)\n",
    "    res_pose = res_pose[:num_frames, :]\n",
    "    while num_frames < 48:\n",
    "        res_pose = np.concatenate((res_pose, np.expand_dims(res_pose[-1, :], axis=0)), axis=0)\n",
    "        num_frames += 1\n",
    "\n",
    "    # Append the subsequence to the list of sequences\n",
    "    sequences.append(np.concatenate((res_pose, res_lh, res_rh), axis=1))\n",
    "    # Append the label to the list of labels\n",
    "\n",
    "    # Convert the lists of sequences and labels to numpy arrays\n",
    "    X = np.array(sequences)\n",
    "    #///////////////////////////////////////////////////////////////////////////////////////\n",
    "    interpreterBank.set_tensor(input_detailsBank[0]['index'], X.astype(np.float32))\n",
    "    interpreterBank.invoke()\n",
    "    output_data = interpreterBank.get_tensor(output_detailsBank[0]['index'])\n",
    "    result = np.argmax(output_data[0])\n",
    "    word = np.array(['on', 'Bank manager', 'Contract', 'Visa', 'Driving License', 'account', 'receive', 'in', 'money', 'my', 'Thank god', 'birth certificate', 'Withdraw money', 'doing', 'finance', 'Bank', 'loan', 'Form', 'Cairo', 'Shop License', 'deaf', 'deposit', 'rent', 'your', 'and', 'private', 'ownership', 'what', 'monthly payment', 'conditions', 'adress', 'project', 'authorization', 'ok', 'hello', 'help', 'Integrated services card', 'or', 'branch', 'question', 'Revenues', 'housing', 'salary', 'fine', 'Financial benefits', 'changing', 'open', 'Your health is fineΓò¬╞Æ', 'How much', 'because', 'Id card', 'problem', 'wait', 'expenses', 'need', 'pension', 'me', 'you', 'yes', 'rejection', 'savings book', 'Alexandria', 'apartment', 'How are you', 'money transfer', 'name', 'support', 'window', 'qualification', 'general', 'new', 'governorate', 'Death certificate', 'Sign language interpreter', 'signature', 'number'])\n",
    "    return jsonify({'placement':str(word[result])})\n",
    "\n",
    "\n",
    "@app.route('/predictCafe',methods=['POST'])\n",
    "def predictCafe():\n",
    "    \n",
    "#     frames = request.body('Frames')\n",
    "    data = request.get_json()\n",
    "    frames = data['Frames']\n",
    "    framelist = []\n",
    "    counter = 0\n",
    "    # Loop over the list and process each bitmap object\n",
    "    for frame in frames:\n",
    "        image_value={}\n",
    "        for key, value in frame.items():\n",
    "            image_value = value\n",
    "        image_data = base64.b64decode(image_value['Image'])\n",
    "        img = Image.open(BytesIO(image_data))\n",
    "        image_path = f\"{app.config['UPLOAD_FOLDER']}{counter}.jpg\"\n",
    "        counter+=1\n",
    "        img.save(image_path, format=\"JPEG\")\n",
    "        framelist.append(f'{image_path}')\n",
    "#     counter = 0\n",
    "#     pathToFrames = []\n",
    "#     for frame in frames:\n",
    "#         filepath = os.path.join(app.config['UPLOAD_FOLDER'], f'{counter}.bmp')\n",
    "#         frame.save(filepath)\n",
    "#         pathToFrames.append(filepath)\n",
    "#         counter++\n",
    "#     framList = sorted(os.listdir(os.path.join(app.config['UPLOAD_FOLDER'])))\n",
    "    mp_holistic = mp.solutions.holistic\n",
    "    #_________________________________________________\n",
    "#     framList = []\n",
    "#     video_capture = cv2.VideoCapture(vid_file)\n",
    "#     ret, frame = video_capture.read()\n",
    "#     while ret:\n",
    "#         if frame is not None:\n",
    "#             frame = cv2.resize(frame, (224, 224), interpolation=cv2.INTER_AREA)\n",
    "#             framList.append(frame)\n",
    "#         ret, frame = video_capture.read()\n",
    "#     video_capture.release()\n",
    "    #________________________________________________\n",
    "    #*******************************************************************\n",
    "    pose_keypoints, lh_keypoints, rh_keypoints = [], [], []\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "      # Loop through the video frames\n",
    "      for frame in framelist:\n",
    "    #         image, results = mediapipe_detection(frame, holistic)\n",
    "            #///////////////////////////////////////////////////////\n",
    "            image = cv2.imread(frame)\n",
    "            if image is None:\n",
    "                raise ValueError(f\"Failed to load image: {image}\")\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "            image.flags.writeable = False                  # Image is no longer writeable\n",
    "            results = holistic.process(image)                 # Make prediction\n",
    "            image.flags.writeable = True                   # Image is now writeable\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "            #///////////////////////////////////////////////////////\n",
    "\n",
    "            # Extract keypoints\n",
    "    #         pose, lh, rh = extract_keypoints(results)\n",
    "            #>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "            pose = np.array([[res.x, res.y, res.z] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*3)\n",
    "            lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "            rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "            nose=pose[:3]\n",
    "            lh_wrist=lh[:3]\n",
    "            rh_wrist=rh[:3]\n",
    "    #         pose = adjust_landmarks(pose,nose)\n",
    "            ###########################################################\n",
    "            arr_reshaped = pose.reshape(-1, 3)\n",
    "            center_repeated = np.tile(nose, (len(arr_reshaped), 1))\n",
    "            arr_adjusted = arr_reshaped - center_repeated\n",
    "            pose = arr_adjusted.reshape(-1)\n",
    "            ###########################################################\n",
    "    #         lh = adjust_landmarks(lh,lh_wrist)\n",
    "            ###########################################################\n",
    "            arr_reshaped = lh.reshape(-1, 3)\n",
    "            center_repeated = np.tile(lh_wrist, (len(arr_reshaped), 1))\n",
    "            arr_adjusted = arr_reshaped - center_repeated\n",
    "            lh = arr_adjusted.reshape(-1)\n",
    "            ###########################################################\n",
    "    #         rh = adjust_landmarks(rh,rh_wrist)\n",
    "            ###########################################################\n",
    "            arr_reshaped = rh.reshape(-1, 3)\n",
    "            center_repeated = np.tile(rh_wrist, (len(arr_reshaped), 1))\n",
    "            arr_adjusted = arr_reshaped - center_repeated\n",
    "            rh = arr_adjusted.reshape(-1)\n",
    "            ###########################################################\n",
    "            #>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>.\n",
    "            # Add the keypoints to the list for this video\n",
    "            pose_keypoints.append(pose)\n",
    "            lh_keypoints.append(lh)\n",
    "            rh_keypoints.append(rh)     \n",
    "    #*********************************************************************\n",
    "    # Initialize the lists of sequences and labels\n",
    "    sequences = []\n",
    "    labels = []\n",
    "\n",
    "    # Iterate over the words\n",
    "\n",
    "    # Iterate through the sequences (numpy arrays) contained in the lh_keypoints folder\n",
    "    # Load the left hand array\n",
    "    res_lh = np.array(lh_keypoints)\n",
    "\n",
    "    # Determine how many frames to select\n",
    "    num_frames = min(res_lh.shape[0], 48)\n",
    "    res_lh = res_lh[:num_frames, :]\n",
    "    while num_frames < 48:\n",
    "        res_lh = np.concatenate((res_lh, np.expand_dims(res_lh[-1, :], axis=0)), axis=0)\n",
    "        num_frames += 1\n",
    "\n",
    "    # Load the right hand array\n",
    "    res_rh =  np.array(rh_keypoints)\n",
    "\n",
    "    # Determine how many frames to select\n",
    "    num_frames = min(res_rh.shape[0], 48)\n",
    "    res_rh = res_rh[:num_frames, :]\n",
    "    while num_frames < 48:\n",
    "        res_rh = np.concatenate((res_rh, np.expand_dims(res_rh[-1, :], axis=0)), axis=0)\n",
    "        num_frames += 1\n",
    "\n",
    "    # Load the pose array\n",
    "    res_pose =np.array(pose_keypoints) \n",
    "\n",
    "    # Determine how many frames to select\n",
    "    num_frames = min(res_pose.shape[0], 48)\n",
    "    res_pose = res_pose[:num_frames, :]\n",
    "    while num_frames < 48:\n",
    "        res_pose = np.concatenate((res_pose, np.expand_dims(res_pose[-1, :], axis=0)), axis=0)\n",
    "        num_frames += 1\n",
    "\n",
    "    # Append the subsequence to the list of sequences\n",
    "    sequences.append(np.concatenate((res_pose, res_lh, res_rh), axis=1))\n",
    "    # Append the label to the list of labels\n",
    "\n",
    "    # Convert the lists of sequences and labels to numpy arrays\n",
    "    X = np.array(sequences)\n",
    "    #///////////////////////////////////////////////////////////////////////////////////////\n",
    "    interpreterCafe.set_tensor(input_detailsCafe[0]['index'], X.astype(np.float32))\n",
    "    interpreterCafe.invoke()\n",
    "    output_data = interpreterCafe.get_tensor(output_detailsCafe[0]['index'])\n",
    "    result = np.argmax(output_data[0])\n",
    "    word = np.array(['fine', 'glass', 'ok', 'me', 'in', 'milk', 'water', 'banana', 'cold', 'drink', 'How much', 'question', 'adress', 'no', 'anise', 'cinnamon', 'name', 'berell', 'hello', 'you', 'number', 'hot', 'Thank god', 'doing', 'How are you', 'juice', 'wait', 'nescafe', 'lemon', 'need', 'on', 'problem', 'pepsi', 'tea', 'spoon', 'deaf', 'mint', 'mango', 'help', 'or', 'orange', 'coffee', 'your', 'and', 'Your health is fine╪ƒ', 'Roselle', 'what', 'new', 'my', 'money', 'strawberry', 'yes', 'birth certificate', 'fenugreek'])\n",
    "    return jsonify({'placement':str(word[result])})\n",
    "\n",
    "\n",
    "\n",
    "@app.route('/predictTrain',methods=['POST'])\n",
    "def predictTrain():\n",
    "    \n",
    "#     frames = request.body('Frames')\n",
    "    data = request.get_json()\n",
    "    frames = data['Frames']\n",
    "    framelist = []\n",
    "    counter = 0\n",
    "    # Loop over the list and process each bitmap object\n",
    "    for frame in frames:\n",
    "        image_value={}\n",
    "        for key, value in frame.items():\n",
    "            image_value = value\n",
    "        image_data = base64.b64decode(image_value['Image'])\n",
    "        img = Image.open(BytesIO(image_data))\n",
    "        image_path = f\"{app.config['UPLOAD_FOLDER']}{counter}.jpg\"\n",
    "        counter+=1\n",
    "        img.save(image_path, format=\"JPEG\")\n",
    "        framelist.append(f'{image_path}')\n",
    "#     counter = 0\n",
    "#     pathToFrames = []\n",
    "#     for frame in frames:\n",
    "#         filepath = os.path.join(app.config['UPLOAD_FOLDER'], f'{counter}.bmp')\n",
    "#         frame.save(filepath)\n",
    "#         pathToFrames.append(filepath)\n",
    "#         counter++\n",
    "#     framList = sorted(os.listdir(os.path.join(app.config['UPLOAD_FOLDER'])))\n",
    "    mp_holistic = mp.solutions.holistic\n",
    "    #_________________________________________________\n",
    "#     framList = []\n",
    "#     video_capture = cv2.VideoCapture(vid_file)\n",
    "#     ret, frame = video_capture.read()\n",
    "#     while ret:\n",
    "#         if frame is not None:\n",
    "#             frame = cv2.resize(frame, (224, 224), interpolation=cv2.INTER_AREA)\n",
    "#             framList.append(frame)\n",
    "#         ret, frame = video_capture.read()\n",
    "#     video_capture.release()\n",
    "    #________________________________________________\n",
    "    #*******************************************************************\n",
    "    pose_keypoints, lh_keypoints, rh_keypoints = [], [], []\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "      # Loop through the video frames\n",
    "      for frame in framelist:\n",
    "    #         image, results = mediapipe_detection(frame, holistic)\n",
    "            #///////////////////////////////////////////////////////\n",
    "            image = cv2.imread(frame)\n",
    "            if image is None:\n",
    "                raise ValueError(f\"Failed to load image: {image}\")\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "            image.flags.writeable = False                  # Image is no longer writeable\n",
    "            results = holistic.process(image)                 # Make prediction\n",
    "            image.flags.writeable = True                   # Image is now writeable\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "            #///////////////////////////////////////////////////////\n",
    "\n",
    "            # Extract keypoints\n",
    "    #         pose, lh, rh = extract_keypoints(results)\n",
    "            #>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "            pose = np.array([[res.x, res.y, res.z] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*3)\n",
    "            lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "            rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "            nose=pose[:3]\n",
    "            lh_wrist=lh[:3]\n",
    "            rh_wrist=rh[:3]\n",
    "    #         pose = adjust_landmarks(pose,nose)\n",
    "            ###########################################################\n",
    "            arr_reshaped = pose.reshape(-1, 3)\n",
    "            center_repeated = np.tile(nose, (len(arr_reshaped), 1))\n",
    "            arr_adjusted = arr_reshaped - center_repeated\n",
    "            pose = arr_adjusted.reshape(-1)\n",
    "            ###########################################################\n",
    "    #         lh = adjust_landmarks(lh,lh_wrist)\n",
    "            ###########################################################\n",
    "            arr_reshaped = lh.reshape(-1, 3)\n",
    "            center_repeated = np.tile(lh_wrist, (len(arr_reshaped), 1))\n",
    "            arr_adjusted = arr_reshaped - center_repeated\n",
    "            lh = arr_adjusted.reshape(-1)\n",
    "            ###########################################################\n",
    "    #         rh = adjust_landmarks(rh,rh_wrist)\n",
    "            ###########################################################\n",
    "            arr_reshaped = rh.reshape(-1, 3)\n",
    "            center_repeated = np.tile(rh_wrist, (len(arr_reshaped), 1))\n",
    "            arr_adjusted = arr_reshaped - center_repeated\n",
    "            rh = arr_adjusted.reshape(-1)\n",
    "            ###########################################################\n",
    "            #>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>.\n",
    "            # Add the keypoints to the list for this video\n",
    "            pose_keypoints.append(pose)\n",
    "            lh_keypoints.append(lh)\n",
    "            rh_keypoints.append(rh)     \n",
    "    #*********************************************************************\n",
    "    # Initialize the lists of sequences and labels\n",
    "    sequences = []\n",
    "    labels = []\n",
    "\n",
    "    # Iterate over the words\n",
    "\n",
    "    # Iterate through the sequences (numpy arrays) contained in the lh_keypoints folder\n",
    "    # Load the left hand array\n",
    "    res_lh = np.array(lh_keypoints)\n",
    "\n",
    "    # Determine how many frames to select\n",
    "    num_frames = min(res_lh.shape[0], 48)\n",
    "    res_lh = res_lh[:num_frames, :]\n",
    "    while num_frames < 48:\n",
    "        res_lh = np.concatenate((res_lh, np.expand_dims(res_lh[-1, :], axis=0)), axis=0)\n",
    "        num_frames += 1\n",
    "\n",
    "    # Load the right hand array\n",
    "    res_rh =  np.array(rh_keypoints)\n",
    "\n",
    "    # Determine how many frames to select\n",
    "    num_frames = min(res_rh.shape[0], 48)\n",
    "    res_rh = res_rh[:num_frames, :]\n",
    "    while num_frames < 48:\n",
    "        res_rh = np.concatenate((res_rh, np.expand_dims(res_rh[-1, :], axis=0)), axis=0)\n",
    "        num_frames += 1\n",
    "\n",
    "    # Load the pose array\n",
    "    res_pose =np.array(pose_keypoints) \n",
    "\n",
    "    # Determine how many frames to select\n",
    "    num_frames = min(res_pose.shape[0], 48)\n",
    "    res_pose = res_pose[:num_frames, :]\n",
    "    while num_frames < 48:\n",
    "        res_pose = np.concatenate((res_pose, np.expand_dims(res_pose[-1, :], axis=0)), axis=0)\n",
    "        num_frames += 1\n",
    "\n",
    "    # Append the subsequence to the list of sequences\n",
    "    sequences.append(np.concatenate((res_pose, res_lh, res_rh), axis=1))\n",
    "    # Append the label to the list of labels\n",
    "\n",
    "    # Convert the lists of sequences and labels to numpy arrays\n",
    "    X = np.array(sequences)\n",
    "    #///////////////////////////////////////////////////////////////////////////////////////\n",
    "    interpreterTrain.set_tensor(input_detailsTrain[0]['index'], X.astype(np.float32))\n",
    "    interpreterTrain.invoke()\n",
    "    output_data = interpreterTrain.get_tensor(output_detailsTrain[0]['index'])\n",
    "    result = np.argmax(output_data[0])\n",
    "    word = np.array(['Banha', 'Train station', 'Ismailia', 'Id card', 'question', 'Damanhur', 'Thank god', 'Tanta', 'Kafr El-Sheikh', 'discount', 'Mansoura', 'How much', 'Sign language interpreter', 'problem', 'on', 'Qena', 'or', 'Sohag', 'Aswan', 'ok', 'Giza', 'Alexandria', 'because', 'Port Said', 'Marsa Matrouh', 'where', 'me', 'delay', 'help', 'what', 'fast', 'money', 'I go', 'new', 'Bani Sweif', 'in', 'my', 'yes', 'doing', 'hour', 'hello', 'deaf', 'number', 'and', 'wait', 'after', 'Integrated services card', 'How are you', 'Damietta', 'before', 'luxor', 'place', 'late', 'Cairo', 'ticket', 'adress', 'Train', 'He will come', 'birth certificate', 'together', 'need', 'name', 'Form', 'Fayoum', 'Suez', 'you', 'fine', 'Minya', 'when', 'Asyut', 'your'])\n",
    "    return jsonify({'placement':str(word[result])})\n",
    "\n",
    "\n",
    "\n",
    "@app.route('/predictHospital',methods=['POST'])\n",
    "def predictHospital():\n",
    "    \n",
    "#     frames = request.body('Frames')\n",
    "    data = request.get_json()\n",
    "    frames = data['Frames']\n",
    "    framelist = []\n",
    "    counter = 0\n",
    "    # Loop over the list and process each bitmap object\n",
    "    for frame in frames:\n",
    "        image_value={}\n",
    "        for key, value in frame.items():\n",
    "            image_value = value\n",
    "        image_data = base64.b64decode(image_value['Image'])\n",
    "        img = Image.open(BytesIO(image_data))\n",
    "        image_path = f\"{app.config['UPLOAD_FOLDER']}{counter}.jpg\"\n",
    "        counter+=1\n",
    "        img.save(image_path, format=\"JPEG\")\n",
    "        framelist.append(f'{image_path}')\n",
    "#     counter = 0\n",
    "#     pathToFrames = []\n",
    "#     for frame in frames:\n",
    "#         filepath = os.path.join(app.config['UPLOAD_FOLDER'], f'{counter}.bmp')\n",
    "#         frame.save(filepath)\n",
    "#         pathToFrames.append(filepath)\n",
    "#         counter++\n",
    "#     framList = sorted(os.listdir(os.path.join(app.config['UPLOAD_FOLDER'])))\n",
    "    mp_holistic = mp.solutions.holistic\n",
    "    #_________________________________________________\n",
    "#     framList = []\n",
    "#     video_capture = cv2.VideoCapture(vid_file)\n",
    "#     ret, frame = video_capture.read()\n",
    "#     while ret:\n",
    "#         if frame is not None:\n",
    "#             frame = cv2.resize(frame, (224, 224), interpolation=cv2.INTER_AREA)\n",
    "#             framList.append(frame)\n",
    "#         ret, frame = video_capture.read()\n",
    "#     video_capture.release()\n",
    "    #________________________________________________\n",
    "    #*******************************************************************\n",
    "    pose_keypoints, lh_keypoints, rh_keypoints = [], [], []\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "      # Loop through the video frames\n",
    "      for frame in framelist:\n",
    "    #         image, results = mediapipe_detection(frame, holistic)\n",
    "            #///////////////////////////////////////////////////////\n",
    "            image = cv2.imread(frame)\n",
    "            if image is None:\n",
    "                raise ValueError(f\"Failed to load image: {image}\")\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "            image.flags.writeable = False                  # Image is no longer writeable\n",
    "            results = holistic.process(image)                 # Make prediction\n",
    "            image.flags.writeable = True                   # Image is now writeable\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "            #///////////////////////////////////////////////////////\n",
    "\n",
    "            # Extract keypoints\n",
    "    #         pose, lh, rh = extract_keypoints(results)\n",
    "            #>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "            pose = np.array([[res.x, res.y, res.z] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*3)\n",
    "            lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "            rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "            nose=pose[:3]\n",
    "            lh_wrist=lh[:3]\n",
    "            rh_wrist=rh[:3]\n",
    "    #         pose = adjust_landmarks(pose,nose)\n",
    "            ###########################################################\n",
    "            arr_reshaped = pose.reshape(-1, 3)\n",
    "            center_repeated = np.tile(nose, (len(arr_reshaped), 1))\n",
    "            arr_adjusted = arr_reshaped - center_repeated\n",
    "            pose = arr_adjusted.reshape(-1)\n",
    "            ###########################################################\n",
    "    #         lh = adjust_landmarks(lh,lh_wrist)\n",
    "            ###########################################################\n",
    "            arr_reshaped = lh.reshape(-1, 3)\n",
    "            center_repeated = np.tile(lh_wrist, (len(arr_reshaped), 1))\n",
    "            arr_adjusted = arr_reshaped - center_repeated\n",
    "            lh = arr_adjusted.reshape(-1)\n",
    "            ###########################################################\n",
    "    #         rh = adjust_landmarks(rh,rh_wrist)\n",
    "            ###########################################################\n",
    "            arr_reshaped = rh.reshape(-1, 3)\n",
    "            center_repeated = np.tile(rh_wrist, (len(arr_reshaped), 1))\n",
    "            arr_adjusted = arr_reshaped - center_repeated\n",
    "            rh = arr_adjusted.reshape(-1)\n",
    "            ###########################################################\n",
    "            #>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>.\n",
    "            # Add the keypoints to the list for this video\n",
    "            pose_keypoints.append(pose)\n",
    "            lh_keypoints.append(lh)\n",
    "            rh_keypoints.append(rh)     \n",
    "    #*********************************************************************\n",
    "    # Initialize the lists of sequences and labels\n",
    "    sequences = []\n",
    "    labels = []\n",
    "\n",
    "    # Iterate over the words\n",
    "\n",
    "    # Iterate through the sequences (numpy arrays) contained in the lh_keypoints folder\n",
    "    # Load the left hand array\n",
    "    res_lh = np.array(lh_keypoints)\n",
    "\n",
    "    # Determine how many frames to select\n",
    "    num_frames = min(res_lh.shape[0], 48)\n",
    "    res_lh = res_lh[:num_frames, :]\n",
    "    while num_frames < 48:\n",
    "        res_lh = np.concatenate((res_lh, np.expand_dims(res_lh[-1, :], axis=0)), axis=0)\n",
    "        num_frames += 1\n",
    "\n",
    "    # Load the right hand array\n",
    "    res_rh =  np.array(rh_keypoints)\n",
    "\n",
    "    # Determine how many frames to select\n",
    "    num_frames = min(res_rh.shape[0], 48)\n",
    "    res_rh = res_rh[:num_frames, :]\n",
    "    while num_frames < 48:\n",
    "        res_rh = np.concatenate((res_rh, np.expand_dims(res_rh[-1, :], axis=0)), axis=0)\n",
    "        num_frames += 1\n",
    "\n",
    "    # Load the pose array\n",
    "    res_pose =np.array(pose_keypoints) \n",
    "\n",
    "    # Determine how many frames to select\n",
    "    num_frames = min(res_pose.shape[0], 48)\n",
    "    res_pose = res_pose[:num_frames, :]\n",
    "    while num_frames < 48:\n",
    "        res_pose = np.concatenate((res_pose, np.expand_dims(res_pose[-1, :], axis=0)), axis=0)\n",
    "        num_frames += 1\n",
    "\n",
    "    # Append the subsequence to the list of sequences\n",
    "    sequences.append(np.concatenate((res_pose, res_lh, res_rh), axis=1))\n",
    "    # Append the label to the list of labels\n",
    "\n",
    "    # Convert the lists of sequences and labels to numpy arrays\n",
    "    X = np.array(sequences)\n",
    "    #///////////////////////////////////////////////////////////////////////////////////////\n",
    "    interpreterHospital.set_tensor(input_detailsHospital[0]['index'], X.astype(np.float32))\n",
    "    interpreterHospital.invoke()\n",
    "    output_data = interpreterHospital.get_tensor(output_detailsHospital[0]['index'])\n",
    "    result = np.argmax(output_data[0])\n",
    "    word = np.array(['Obstetrics and gynecology', 'and', 'what', 'overheat', 'on', 'hospital', 'feed', 'ointment', 'Form', 'rays', 'Congestion', 'burn', 'eyes', 'room', 'How are you', 'gauze', 'illness', 'vacation', 'my', 'nusre', 'adress', 'syrup', 'service', 'new', 'heart', 'Sign language interpreter', 'Diabetes', 'birth', 'physical therapy', 'deaf', 'Tell me', 'doctor', 'Anemia', 'sick', 'brain', 'blood', 'because', 'Health Care', 'Forbidden', 'pee', 'tired', 'Thank god', 'headache', 'your', 'Diarrhea', 'ok', 'ticket', 'colic', 'skin', 'Ear, Nose and Throat', 'fracture', 'or', 'poop', 'wait', 'doing', 'question', 'number', 'in', 'you', 'name', 'corona', 'Emergency', 'Injection', 'health insurance', 'scissors', 'nerves', 'problem', 'fine', 'birth certificate', 'tab', 'analysis', 'vertigo', 'bones', 'money', 'Recepion', 'Pregnancy', 'How much', 'liver', 'appendicitis', 'Id card', 'mask', 'hello', 'cold', 'canula', 'cotton', 'model', 'pressure', 'children', 'surgery', 'Your health is fineΓò¬╞Æ', 'paper', 'Poisoning', 'section', 'virus', 'me', 'constipation', 'esoteric', 'Heart attack', 'Integrated services card', 'yes', 'help', 'need'])\n",
    "    return jsonify({'placement':str(word[result])})\n",
    "\n",
    "\n",
    "\n",
    "#Greek\n",
    "interpreterGreek = tf.lite.Interpreter(model_path=model_fileGreek)\n",
    "interpreterGreek.allocate_tensors()\n",
    "input_detailsGreek = interpreterGreek.get_input_details()\n",
    "output_detailsGreek = interpreterGreek.get_output_details()\n",
    "\n",
    "@app.route('/predictGreek',methods=['POST'])\n",
    "def predictGreek():\n",
    "    \n",
    "#     frames = request.body('Frames')\n",
    "    data = request.get_json()\n",
    "    frames = data['Frames']\n",
    "    framelist = []\n",
    "    counter = 0\n",
    "    # Loop over the list and process each bitmap object\n",
    "    for frame in frames:\n",
    "        image_value={}\n",
    "        for key, value in frame.items():\n",
    "            image_value = value\n",
    "        image_data = base64.b64decode(image_value['Image'])\n",
    "        img = Image.open(BytesIO(image_data))\n",
    "        image_path = f\"{app.config['UPLOAD_FOLDER']}{counter}.jpg\"\n",
    "        counter+=1\n",
    "        img.save(image_path, format=\"JPEG\")\n",
    "        framelist.append(f'{image_path}')\n",
    "#     counter = 0\n",
    "#     pathToFrames = []\n",
    "#     for frame in frames:\n",
    "#         filepath = os.path.join(app.config['UPLOAD_FOLDER'], f'{counter}.bmp')\n",
    "#         frame.save(filepath)\n",
    "#         pathToFrames.append(filepath)\n",
    "#         counter++\n",
    "#     framList = sorted(os.listdir(os.path.join(app.config['UPLOAD_FOLDER'])))\n",
    "    mp_holistic = mp.solutions.holistic\n",
    "    #_________________________________________________\n",
    "#     framList = []\n",
    "#     video_capture = cv2.VideoCapture(vid_file)\n",
    "#     ret, frame = video_capture.read()\n",
    "#     while ret:\n",
    "#         if frame is not None:\n",
    "#             frame = cv2.resize(frame, (224, 224), interpolation=cv2.INTER_AREA)\n",
    "#             framList.append(frame)\n",
    "#         ret, frame = video_capture.read()\n",
    "#     video_capture.release()\n",
    "    #________________________________________________\n",
    "    #*******************************************************************\n",
    "    pose_keypoints, lh_keypoints, rh_keypoints = [], [], []\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "      # Loop through the video frames\n",
    "      for frame in framelist:\n",
    "    #         image, results = mediapipe_detection(frame, holistic)\n",
    "            #///////////////////////////////////////////////////////\n",
    "            image = cv2.imread(frame)\n",
    "            if image is None:\n",
    "                raise ValueError(f\"Failed to load image: {image}\")\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "            image.flags.writeable = False                  # Image is no longer writeable\n",
    "            results = holistic.process(image)                 # Make prediction\n",
    "            image.flags.writeable = True                   # Image is now writeable\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "            #///////////////////////////////////////////////////////\n",
    "\n",
    "            # Extract keypoints\n",
    "    #         pose, lh, rh = extract_keypoints(results)\n",
    "            #>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "            pose = np.array([[res.x, res.y, res.z] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*3)\n",
    "            lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "            rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "            nose=pose[:3]\n",
    "            lh_wrist=lh[:3]\n",
    "            rh_wrist=rh[:3]\n",
    "    #         pose = adjust_landmarks(pose,nose)\n",
    "            ###########################################################\n",
    "            arr_reshaped = pose.reshape(-1, 3)\n",
    "            center_repeated = np.tile(nose, (len(arr_reshaped), 1))\n",
    "            arr_adjusted = arr_reshaped - center_repeated\n",
    "            pose = arr_adjusted.reshape(-1)\n",
    "            ###########################################################\n",
    "    #         lh = adjust_landmarks(lh,lh_wrist)\n",
    "            ###########################################################\n",
    "            arr_reshaped = lh.reshape(-1, 3)\n",
    "            center_repeated = np.tile(lh_wrist, (len(arr_reshaped), 1))\n",
    "            arr_adjusted = arr_reshaped - center_repeated\n",
    "            lh = arr_adjusted.reshape(-1)\n",
    "            ###########################################################\n",
    "    #         rh = adjust_landmarks(rh,rh_wrist)\n",
    "            ###########################################################\n",
    "            arr_reshaped = rh.reshape(-1, 3)\n",
    "            center_repeated = np.tile(rh_wrist, (len(arr_reshaped), 1))\n",
    "            arr_adjusted = arr_reshaped - center_repeated\n",
    "            rh = arr_adjusted.reshape(-1)\n",
    "            ###########################################################\n",
    "            #>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>.\n",
    "            # Add the keypoints to the list for this video\n",
    "            pose_keypoints.append(pose)\n",
    "            lh_keypoints.append(lh)\n",
    "            rh_keypoints.append(rh)     \n",
    "    #*********************************************************************\n",
    "    # Initialize the lists of sequences and labels\n",
    "    sequences = []\n",
    "    labels = []\n",
    "\n",
    "    # Iterate over the words\n",
    "\n",
    "    # Iterate through the sequences (numpy arrays) contained in the lh_keypoints folder\n",
    "    # Load the left hand array\n",
    "    res_lh = np.array(lh_keypoints)\n",
    "\n",
    "    # Determine how many frames to select\n",
    "    num_frames = min(res_lh.shape[0], 48)\n",
    "    res_lh = res_lh[:num_frames, :]\n",
    "    while num_frames < 48:\n",
    "        res_lh = np.concatenate((res_lh, np.expand_dims(res_lh[-1, :], axis=0)), axis=0)\n",
    "        num_frames += 1\n",
    "\n",
    "    # Load the right hand array\n",
    "    res_rh =  np.array(rh_keypoints)\n",
    "\n",
    "    # Determine how many frames to select\n",
    "    num_frames = min(res_rh.shape[0], 48)\n",
    "    res_rh = res_rh[:num_frames, :]\n",
    "    while num_frames < 48:\n",
    "        res_rh = np.concatenate((res_rh, np.expand_dims(res_rh[-1, :], axis=0)), axis=0)\n",
    "        num_frames += 1\n",
    "\n",
    "    # Load the pose array\n",
    "    res_pose =np.array(pose_keypoints) \n",
    "\n",
    "    # Determine how many frames to select\n",
    "    num_frames = min(res_pose.shape[0], 48)\n",
    "    res_pose = res_pose[:num_frames, :]\n",
    "    while num_frames < 48:\n",
    "        res_pose = np.concatenate((res_pose, np.expand_dims(res_pose[-1, :], axis=0)), axis=0)\n",
    "        num_frames += 1\n",
    "\n",
    "    # Append the subsequence to the list of sequences\n",
    "    sequences.append(np.concatenate((res_pose, res_lh, res_rh), axis=1))\n",
    "    # Append the label to the list of labels\n",
    "\n",
    "    # Convert the lists of sequences and labels to numpy arrays\n",
    "    X = np.array(sequences)\n",
    "    interpreterGreek.set_tensor(input_detailsGreek[0]['index'], X.astype(np.float32))\n",
    "    interpreterGreek.invoke()\n",
    "    output_data = interpreterGreek.get_tensor(output_detailsGreek[0]['index'])\n",
    "    result = np.argmax(output_data[0])\n",
    "    return jsonify({'placement':str(getcodeGreek(result))})\n",
    "\n",
    "\n",
    "#tur\n",
    "\n",
    "@app.route('/predictTur',methods=['POST'])\n",
    "def predictTur():\n",
    "    interpreterTur = tf.lite.Interpreter(model_path=model_fileTur)\n",
    "    interpreterTur.allocate_tensors()\n",
    "    input_detailsTur = interpreterTur.get_input_details()\n",
    "    output_detailsTur = interpreterTur.get_output_details()\n",
    "#     frames = request.body('Frames')\n",
    "    data = request.get_json()\n",
    "    frames = data['Frames']\n",
    "    framelist = []\n",
    "    counter = 0\n",
    "    # Loop over the list and process each bitmap object\n",
    "    for frame in frames:\n",
    "        image_value={}\n",
    "        for key, value in frame.items():\n",
    "            image_value = value\n",
    "        image_data = base64.b64decode(image_value['Image'])\n",
    "        img = Image.open(BytesIO(image_data))\n",
    "        image_path = f\"{app.config['UPLOAD_FOLDER']}{counter}.jpg\"\n",
    "        counter+=1\n",
    "        img.save(image_path, format=\"JPEG\")\n",
    "        framelist.append(f'{image_path}')\n",
    "    mp_holistic = mp.solutions.holistic\n",
    "    pose_keypoints, lh_keypoints, rh_keypoints = [], [], []\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "      # Loop through the video frames\n",
    "      for frame in framelist:\n",
    "    #         image, results = mediapipe_detection(frame, holistic)\n",
    "            #///////////////////////////////////////////////////////\n",
    "            image = cv2.imread(frame)\n",
    "            if image is None:\n",
    "                raise ValueError(f\"Failed to load image: {image}\")\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "            image.flags.writeable = False                  # Image is no longer writeable\n",
    "            results = holistic.process(image)                 # Make prediction\n",
    "            image.flags.writeable = True                   # Image is now writeable\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "            #///////////////////////////////////////////////////////\n",
    "\n",
    "            # Extract keypoints\n",
    "    #         pose, lh, rh = extract_keypoints(results)\n",
    "            #>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "            pose = np.array([[res.x, res.y, res.z] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*3)\n",
    "            lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "            rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "            nose=pose[:3]\n",
    "            lh_wrist=lh[:3]\n",
    "            rh_wrist=rh[:3]\n",
    "    #         pose = adjust_landmarks(pose,nose)\n",
    "            ###########################################################\n",
    "            arr_reshaped = pose.reshape(-1, 3)\n",
    "            center_repeated = np.tile(nose, (len(arr_reshaped), 1))\n",
    "            arr_adjusted = arr_reshaped - center_repeated\n",
    "            pose = arr_adjusted.reshape(-1)\n",
    "            ###########################################################\n",
    "    #         lh = adjust_landmarks(lh,lh_wrist)\n",
    "            ###########################################################\n",
    "            arr_reshaped = lh.reshape(-1, 3)\n",
    "            center_repeated = np.tile(lh_wrist, (len(arr_reshaped), 1))\n",
    "            arr_adjusted = arr_reshaped - center_repeated\n",
    "            lh = arr_adjusted.reshape(-1)\n",
    "            ###########################################################\n",
    "    #         rh = adjust_landmarks(rh,rh_wrist)\n",
    "            ###########################################################\n",
    "            arr_reshaped = rh.reshape(-1, 3)\n",
    "            center_repeated = np.tile(rh_wrist, (len(arr_reshaped), 1))\n",
    "            arr_adjusted = arr_reshaped - center_repeated\n",
    "            rh = arr_adjusted.reshape(-1)\n",
    "            ###########################################################\n",
    "            #>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>.\n",
    "            # Add the keypoints to the list for this video\n",
    "            pose_keypoints.append(pose)\n",
    "            lh_keypoints.append(lh)\n",
    "            rh_keypoints.append(rh)     \n",
    "    #*********************************************************************\n",
    "    # Initialize the lists of sequences and labels\n",
    "    sequences = []\n",
    "    labels = []\n",
    "\n",
    "    # Iterate over the words\n",
    "\n",
    "    # Iterate through the sequences (numpy arrays) contained in the lh_keypoints folder\n",
    "    # Load the left hand array\n",
    "    res_lh = np.array(lh_keypoints)\n",
    "\n",
    "    # Determine how many frames to select\n",
    "    num_frames = min(res_lh.shape[0], 48)\n",
    "    res_lh = res_lh[:num_frames, :]\n",
    "    while num_frames < 48:\n",
    "        res_lh = np.concatenate((res_lh, np.expand_dims(res_lh[-1, :], axis=0)), axis=0)\n",
    "        num_frames += 1\n",
    "\n",
    "    # Load the right hand array\n",
    "    res_rh =  np.array(rh_keypoints)\n",
    "\n",
    "    # Determine how many frames to select\n",
    "    num_frames = min(res_rh.shape[0], 48)\n",
    "    res_rh = res_rh[:num_frames, :]\n",
    "    while num_frames < 48:\n",
    "        res_rh = np.concatenate((res_rh, np.expand_dims(res_rh[-1, :], axis=0)), axis=0)\n",
    "        num_frames += 1\n",
    "\n",
    "    # Load the pose array\n",
    "    res_pose =np.array(pose_keypoints) \n",
    "\n",
    "    # Determine how many frames to select\n",
    "    num_frames = min(res_pose.shape[0], 48)\n",
    "    res_pose = res_pose[:num_frames, :]\n",
    "    while num_frames < 48:\n",
    "        res_pose = np.concatenate((res_pose, np.expand_dims(res_pose[-1, :], axis=0)), axis=0)\n",
    "        num_frames += 1\n",
    "\n",
    "    # Append the subsequence to the list of sequences\n",
    "    sequences.append(np.concatenate((res_pose, res_lh, res_rh), axis=1))\n",
    "    # Append the label to the list of labels\n",
    "\n",
    "    # Convert the lists of sequences and labels to numpy arrays\n",
    "    X = np.array(sequences)\n",
    "    interpreterTur.set_tensor(input_detailsTur[0]['index'], X.astype(np.float32))\n",
    "    interpreterTur.invoke()\n",
    "    output_data = interpreterTur.get_tensor(output_detailsTur[0]['index'])\n",
    "    result = np.argmax(output_data[0])\n",
    "    return jsonify({'placement':str(getcodeTur(result))})\n",
    "#______________________________________________________________________\n",
    "@app.route('/predictIndian',methods=['POST'])\n",
    "def predictIndian():\n",
    "    \n",
    "#     frames = request.body('Frames')\n",
    "    data = request.get_json()\n",
    "    frames = data['Frames']\n",
    "    framelist = []\n",
    "    counter = 0\n",
    "    # Loop over the list and process each bitmap object\n",
    "    for frame in frames:\n",
    "        image_value={}\n",
    "        for key, value in frame.items():\n",
    "            image_value = value\n",
    "        image_data = base64.b64decode(image_value['Image'])\n",
    "        img = Image.open(BytesIO(image_data))\n",
    "        image_path = f\"{app.config['UPLOAD_FOLDER']}{counter}.jpg\"\n",
    "        counter+=1\n",
    "        img.save(image_path, format=\"JPEG\")\n",
    "        framelist.append(f'{image_path}')\n",
    "#     counter = 0\n",
    "#     pathToFrames = []\n",
    "#     for frame in frames:\n",
    "#         filepath = os.path.join(app.config['UPLOAD_FOLDER'], f'{counter}.bmp')\n",
    "#         frame.save(filepath)\n",
    "#         pathToFrames.append(filepath)\n",
    "#         counter++\n",
    "#     framList = sorted(os.listdir(os.path.join(app.config['UPLOAD_FOLDER'])))\n",
    "    mp_holistic = mp.solutions.holistic\n",
    "    #_________________________________________________\n",
    "#     framList = []\n",
    "#     video_capture = cv2.VideoCapture(vid_file)\n",
    "#     ret, frame = video_capture.read()\n",
    "#     while ret:\n",
    "#         if frame is not None:\n",
    "#             frame = cv2.resize(frame, (224, 224), interpolation=cv2.INTER_AREA)\n",
    "#             framList.append(frame)\n",
    "#         ret, frame = video_capture.read()\n",
    "#     video_capture.release()\n",
    "    #________________________________________________\n",
    "    #*******************************************************************\n",
    "    pose_keypoints, lh_keypoints, rh_keypoints = [], [], []\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "      # Loop through the video frames\n",
    "      for frame in framelist:\n",
    "    #         image, results = mediapipe_detection(frame, holistic)\n",
    "            #///////////////////////////////////////////////////////\n",
    "            image = cv2.imread(frame)\n",
    "            if image is None:\n",
    "                raise ValueError(f\"Failed to load image: {image}\")\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "            image.flags.writeable = False                  # Image is no longer writeable\n",
    "            results = holistic.process(image)                 # Make prediction\n",
    "            image.flags.writeable = True                   # Image is now writeable\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "            #///////////////////////////////////////////////////////\n",
    "\n",
    "            # Extract keypoints\n",
    "    #         pose, lh, rh = extract_keypoints(results)\n",
    "            #>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "            pose = np.array([[res.x, res.y, res.z] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*3)\n",
    "            lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "            rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "            nose=pose[:3]\n",
    "            lh_wrist=lh[:3]\n",
    "            rh_wrist=rh[:3]\n",
    "    #         pose = adjust_landmarks(pose,nose)\n",
    "            ###########################################################\n",
    "            arr_reshaped = pose.reshape(-1, 3)\n",
    "            center_repeated = np.tile(nose, (len(arr_reshaped), 1))\n",
    "            arr_adjusted = arr_reshaped - center_repeated\n",
    "            pose = arr_adjusted.reshape(-1)\n",
    "            ###########################################################\n",
    "    #         lh = adjust_landmarks(lh,lh_wrist)\n",
    "            ###########################################################\n",
    "            arr_reshaped = lh.reshape(-1, 3)\n",
    "            center_repeated = np.tile(lh_wrist, (len(arr_reshaped), 1))\n",
    "            arr_adjusted = arr_reshaped - center_repeated\n",
    "            lh = arr_adjusted.reshape(-1)\n",
    "            ###########################################################\n",
    "    #         rh = adjust_landmarks(rh,rh_wrist)\n",
    "            ###########################################################\n",
    "            arr_reshaped = rh.reshape(-1, 3)\n",
    "            center_repeated = np.tile(rh_wrist, (len(arr_reshaped), 1))\n",
    "            arr_adjusted = arr_reshaped - center_repeated\n",
    "            rh = arr_adjusted.reshape(-1)\n",
    "            ###########################################################\n",
    "            #>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>.\n",
    "            # Add the keypoints to the list for this video\n",
    "            pose_keypoints.append(pose)\n",
    "            lh_keypoints.append(lh)\n",
    "            rh_keypoints.append(rh)     \n",
    "    #*********************************************************************\n",
    "    # Initialize the lists of sequences and labels\n",
    "    sequences = []\n",
    "    labels = []\n",
    "\n",
    "    # Iterate over the words\n",
    "\n",
    "    # Iterate through the sequences (numpy arrays) contained in the lh_keypoints folder\n",
    "    # Load the left hand array\n",
    "    res_lh = np.array(lh_keypoints)\n",
    "\n",
    "    # Determine how many frames to select\n",
    "    num_frames = min(res_lh.shape[0], 48)\n",
    "    res_lh = res_lh[:num_frames, :]\n",
    "    while num_frames < 48:\n",
    "        res_lh = np.concatenate((res_lh, np.expand_dims(res_lh[-1, :], axis=0)), axis=0)\n",
    "        num_frames += 1\n",
    "\n",
    "    # Load the right hand array\n",
    "    res_rh =  np.array(rh_keypoints)\n",
    "\n",
    "    # Determine how many frames to select\n",
    "    num_frames = min(res_rh.shape[0], 48)\n",
    "    res_rh = res_rh[:num_frames, :]\n",
    "    while num_frames < 48:\n",
    "        res_rh = np.concatenate((res_rh, np.expand_dims(res_rh[-1, :], axis=0)), axis=0)\n",
    "        num_frames += 1\n",
    "\n",
    "    # Load the pose array\n",
    "    res_pose =np.array(pose_keypoints) \n",
    "\n",
    "    # Determine how many frames to select\n",
    "    num_frames = min(res_pose.shape[0], 48)\n",
    "    res_pose = res_pose[:num_frames, :]\n",
    "    while num_frames < 48:\n",
    "        res_pose = np.concatenate((res_pose, np.expand_dims(res_pose[-1, :], axis=0)), axis=0)\n",
    "        num_frames += 1\n",
    "\n",
    "    # Append the subsequence to the list of sequences\n",
    "    sequences.append(np.concatenate((res_pose, res_lh, res_rh), axis=1))\n",
    "    # Append the label to the list of labels\n",
    "\n",
    "    # Convert the lists of sequences and labels to numpy arrays\n",
    "    X = np.array(sequences)\n",
    "    interpreterInd.set_tensor(input_detailsInd[0]['index'], X.astype(np.float32))\n",
    "    interpreterInd.invoke()\n",
    "    output_data = interpreterInd.get_tensor(output_detailsInd[0]['index'])\n",
    "    result = np.argmax(output_data[0])\n",
    "    return jsonify({'placement':str(getcodeIndian(result))})\n",
    "\n",
    "def getcodeIndian(n):\n",
    "    code={'Bird': 0, 'Energy': 1, 'Price': 2, 'Race': 3, 'Sign': 4, 'Cat': 5, 'Newspaper': 6, 'War': 7, 'Bill': 8, 'Technology': 9, 'God': 10, 'Fish': 11, 'Peace': 12, 'Horse': 13, 'Team': 14, 'Exercise': 15, 'Death': 16, 'Money': 17, 'Marriage': 18, 'Medicine': 19, 'Gun': 20, 'Cow': 21, 'Mouse': 22, 'Ball': 23, 'Religion': 24, 'Attack': 25, 'Animal': 26, 'Dog': 27, 'Election': 28, 'Science': 29, 'Sport': 30}\n",
    "\n",
    "    for x,y in code.items():\n",
    "        if n==y:\n",
    "            return x\n",
    "\n",
    "\n",
    "def getcodeGreek(n):\n",
    "    code={'note': 0, 'smile': 1, 'rucksack': 2, 'sky': 3, 'send': 4, 'release': 5, 'shelf': 6, 'round': 7, 'remove': 8, 'right': 84, 'fly': 10, 'relative': 11, 'recipe': 12, 'rich': 13, 'quite': 14, 'let': 15, 'RAW': 16, 'sick': 17, 'air': 18, 'rare': 19, 'saturday': 20, 'schedule': 21, '17': 62, 'smell': 23, 'season': 24, 'remember': 25, 'ship': 26, 'return': 27, 'brief': 28, 'say': 29, 'marriage': 31, 'never': 32, 'shout': 33, 'remind': 34, 'small': 35, 'shirt': 36, 'roof': 37, 'sheet': 38, 'on top of': 39, 'seat': 40, 'sell': 41, 'room': 42, 'scientist': 43, 'salt': 44, 'ladder': 45, 'sleep': 46, 'search': 47, 'group': 48, 'share': 51, 'refrigerator': 52, 'sharp': 53, 'short': 54, 'real': 55, 'signal': 56, 'ribbon': 57, 'radiator': 58, 'result': 59, 'from time to time': 60, 'slowly': 61, 'size': 63, 'same': 64, 'save': 65, 'ruin': 66, 'radio': 67, 'sea': 68, 'rain': 69, 'shoe': 70, 'show': 71, 'ring': 72, 'resign': 73, 'rice': 74, 'repeat': 75, 'bag': 76, 'reason': 77, 'sit': 78, 'see': 79, 'serve': 80, 'relation': 81, 'scratch': 82, 'skirt': 83, 'simple': 85, 'responsible': 86, 'river': 87, 'birthday': 88, 'skin': 89, 'drop': 90, 'root': 91, 'similar': 92, 'shop': 93, 'read': 94, 'salad': 95, 'almost': 96, 'reach': 97, 'language': 98, 'shine': 99, 'robust': 100}\n",
    "    for x,y in code.items():\n",
    "        if n==y:\n",
    "            return x\n",
    "def getcodeTur(n):\n",
    "    code={'male': 0, 'enjoy_your_meal': 1, 'gift': 2, 'glass': 3, 'pharmacy': 4, 'change': 5, 'heavy': 6, 'drink': 7, 'tree': 8, 'elephant': 9, 'labor': 10, 'wait': 11, 'government': 12, 'cry': 13, 'meal': 14, 'push': 15, 'key': 16, 'wallet': 17, 'minute': 18, 'not_interested': 19, 'single': 20, 'memorize': 21, 'always': 22, 'saturday': 23, 'tea': 24, 'child': 25, 'together': 26, 'ataturk': 27, 'lightweight': 28, 'honey': 29, 'yesterday': 30, 'yes': 31, 'hammer': 32, 'medicine': 33, 'flag': 34, 'hurry': 35, 'same': 36, 'retired': 37, 'we': 38, 'mirror': 39, 'hospital': 40, 'baby': 41, 'wise': 42, 'animal': 43, 'topple': 44, 'ill': 45, 'laugh': 46, 'shirt': 47, 'football': 48, 'fault': 49, 'ugly': 50, 'unwise': 51, 'wednesday': 52, 'fork': 53, 'soup': 54, 'shoe': 55, 'petrol': 56, 'inform': 57, 'carpet': 58, 'show': 59, 'full': 60, 'friend': 61, 'light': 62, 'glove': 63, 'feast': 64, 'garden': 65, 'wall': 66, 'right': 67, 'sister': 68, 'goodbye': 69, 'no': 70, 'friday': 71, 'towel': 72, 'shopping': 73, 'father': 74, 'enemy': 75, 'halal': 76, 'family': 77, 'mother': 78, 'get_well': 79, 'I': 80, 'grandfather': 81, 'bring': 82, 'past': 83, 'wedding': 84, 'see': 85, 'house': 86, 'kin': 87, 'married': 88, 'needle': 89, 'look': 90, 'work': 91, 'lake': 92, 'hungry': 93, 'photograph': 94, 'brother': 95, 'never': 96, 'teapot': 97, 'congratulations': 98, 'doctor': 99}\n",
    "    for x,y in code.items():\n",
    "        if n==y:\n",
    "            return x\n",
    "def getcode(n):\n",
    "    code={'Ain': 0, 'Al': 1, 'Alef': 2, 'Beh': 3, 'Dad': 4, 'Dal': 5, 'Feh': 6, 'Ghain': 7, 'Hah': 8, 'Heh': 9, 'Jeem': 10, 'Kaf': 11, 'Khah': 12, 'Laa': 13, 'Lam': 14, 'Meem': 15, 'Noon': 16, 'Qaf': 17, 'Reh': 18, 'Sad': 19, 'Seen': 20, 'Sheen': 21, 'Tah': 22, 'Teh': 23, 'Teh_Marbuta': 24, 'Thal': 25, 'Theh': 26, 'Waw': 27, 'Yeh': 28, 'Zah': 29, 'Zain': 30,'nothing' : 31}\n",
    "    for x,y in code.items():\n",
    "        if n==y:\n",
    "            return x\n",
    "def getcodeen(n):\n",
    "    code={'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8, 'J': 9, 'K': 10, 'L': 11, 'M': 12, 'N': 13, 'O': 14, 'P': 15, 'Q': 16, 'R': 17, 'S': 18, 'T': 19, 'U': 20, 'V': 21, 'W': 22, 'X': 23, 'Y': 24, 'Z': 25, 'del': 26, 'nothing': 27, 'space': 28}\n",
    "    for x,y in code.items():\n",
    "        if n==y:\n",
    "            return x\n",
    "\n",
    "@app.route('/words', methods=['POST'])\n",
    "def words():\n",
    "    data = request.get_json()\n",
    "    frames = data['Frames']\n",
    "    framelist = []\n",
    "    counter = 0\n",
    "    # Loop over the list and process each bitmap object\n",
    "    image_value={}\n",
    "    for key, value in frames[0].items():\n",
    "        image_value = value\n",
    "    image_data = base64.b64decode(image_value['Image'])\n",
    "    img = Image.open(BytesIO(image_data))\n",
    "    image_path = f\"{app.config['UPLOAD_FOLDER']}{counter}.jpg\"\n",
    "    counter+=1\n",
    "    img.save(image_path, format=\"JPEG\")\n",
    "    framelist.append(f'{image_path}')\n",
    "    \n",
    "    (input_IMG, output_IMG) = extract_feature(framelist[0])\n",
    "    input_IMG=input_IMG.reshape(-1,1)\n",
    "    input_IMG = np.expand_dims(input_IMG, axis=0)\n",
    "    # Load the TensorFlow Lite model\n",
    "    interpreter = tf.lite.Interpreter(model_path=model_file2)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    # Get input and output tensors\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    input_shape = input_details[0]['shape']\n",
    "    input_data = np.array(input_IMG, dtype=np.float32)\n",
    "    if not np.all(input_data == 0):\n",
    "      # Set input tensor\n",
    "      interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "      # Run inference\n",
    "      interpreter.invoke()\n",
    "\n",
    "      # Get output tensor\n",
    "      output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    \n",
    "      class_index = np.argmax(output_data)\n",
    "    else : \n",
    "        class_index = 31\n",
    "    label=getcode(class_index)\n",
    "    return jsonify({'placement':str(label)})\n",
    "\n",
    "@app.route('/wordsen', methods=['POST'])\n",
    "def wordsen():\n",
    "    data = request.get_json()\n",
    "    frames = data['Frames']\n",
    "    framelist = []\n",
    "    counter = 0\n",
    "    # Loop over the list and process each bitmap object\n",
    "    image_value={}\n",
    "    for key, value in frames[0].items():\n",
    "        image_value = value\n",
    "    image_data = base64.b64decode(image_value['Image'])\n",
    "    img = Image.open(BytesIO(image_data))\n",
    "    image_path = f\"{app.config['UPLOAD_FOLDER']}{counter}.jpg\"\n",
    "    counter+=1\n",
    "    img.save(image_path, format=\"JPEG\")\n",
    "    framelist.append(f'{image_path}')\n",
    "    \n",
    "    (input_IMG, output_IMG) = extract_feature(framelist[0])\n",
    "    input_IMG=input_IMG.reshape(-1,1)\n",
    "    input_IMG = np.expand_dims(input_IMG, axis=0)\n",
    "    # Load the TensorFlow Lite model\n",
    "    interpreter = tf.lite.Interpreter(model_path=model_file3)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    # Get input and output tensors\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    input_shape = input_details[0]['shape']\n",
    "    input_data = np.array(input_IMG, dtype=np.float32)\n",
    "    if not np.all(input_data == 0):\n",
    "      # Set input tensor\n",
    "      interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "      # Run inference\n",
    "      interpreter.invoke()\n",
    "\n",
    "      # Get output tensor\n",
    "      output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    \n",
    "      class_index = np.argmax(output_data)\n",
    "    else : \n",
    "        class_index = 27\n",
    "    label=getcodeen(class_index)\n",
    "    return jsonify({'placement':str(label)})\n",
    "\n",
    "# Function to Extract Feature from images or Frame\n",
    "def extract_feature(input_image):\n",
    "    mp_hands = mp.solutions.hands\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "    image = cv2.imread(input_image)\n",
    "\n",
    "    with mp_hands.Hands(static_image_mode=True, max_num_hands=2, min_detection_confidence=0.1) as hands:\n",
    "        while True:\n",
    "            results = hands.process(cv2.flip(cv2.cvtColor(image, cv2.COLOR_BGR2RGB), 1))\n",
    "            image_height, image_width, _ = image.shape\n",
    "\n",
    "            if not results.multi_hand_landmarks:\n",
    "\n",
    "\n",
    "                hand_data=np.zeros(63)\n",
    "\n",
    "                # Set image to Zero\n",
    "                annotated_image = 0\n",
    "\n",
    "                # Return Whole Landmark and Image\n",
    "                return (hand_data,\n",
    "                        annotated_image)\n",
    "\n",
    "            annotated_image = cv2.flip(image.copy(), 1)\n",
    "            hand_data = []\n",
    "            # Iterate over all hands detected in the image.\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "\n",
    "                # Extract the coordinates of all 21 hand landmarks.\n",
    "                landmark_coordinates = []\n",
    "                for landmark in mp_hands.HandLandmark:\n",
    "                    landmark_coordinates.append(hand_landmarks.landmark[landmark].x * image_width)\n",
    "                    landmark_coordinates.append(hand_landmarks.landmark[landmark].y * image_height)\n",
    "                    landmark_coordinates.append(hand_landmarks.landmark[landmark].z)\n",
    "                mp_drawing.draw_landmarks(annotated_image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "                # Add the coordinates of all 21 hand landmarks for the current hand to the new array.\n",
    "                hand_data.extend(landmark_coordinates)\n",
    "\n",
    "\n",
    "            hand_data = np.array(hand_data)\n",
    "            return  (hand_data, annotated_image)\n",
    "@app.route('/wordstur', methods=['POST'])\n",
    "def wordstur():\n",
    "    np.set_printoptions(precision=3, suppress=True)\n",
    "    data = request.get_json()\n",
    "    frames = data['Frames']\n",
    "    framelist = []\n",
    "    counter = 0\n",
    "    # Loop over the list and process each bitmap object\n",
    "    image_value={}\n",
    "    for key, value in frames[0].items():\n",
    "        image_value = value\n",
    "    image_data = base64.b64decode(image_value['Image'])\n",
    "    img = Image.open(BytesIO(image_data))\n",
    "    image_path = f\"{app.config['UPLOAD_FOLDER']}{counter}.jpg\"\n",
    "    counter+=1\n",
    "    img.save(image_path, format=\"JPEG\")\n",
    "    framelist.append(f'{image_path}')\n",
    "    path_to_image =  framelist[0]\n",
    "    (input_IMG, output_IMG) = extract_featureTur(path_to_image)\n",
    "    input_IMG=input_IMG.reshape(-1,1)\n",
    "    input_IMG = np.expand_dims(input_IMG, axis=0)\n",
    "    # Load the TensorFlow Lite model\n",
    "    interpreter = tf.lite.Interpreter(model_path=modelTur)\n",
    "    interpreter.allocate_tensors()\n",
    "    # Get input and output tensors\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    \n",
    "    # Prepare input data\n",
    "    input_shape = input_details[0]['shape']\n",
    "    input_data = np.array(input_IMG, dtype=np.float32)\n",
    "    if not np.all(input_data == 0):\n",
    "        # Set input tensor\n",
    "        interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "        # Run inference\n",
    "        interpreter.invoke()\n",
    "\n",
    "        # Get output tensor\n",
    "        output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "        out=np.argmax(output_data)\n",
    "    else :\n",
    "        out = 24\n",
    "    return jsonify({'placement':str(getcodeTurk(out))})\n",
    "def getcodeTurk(n):\n",
    "    code={'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8, 'J': 9, 'K': 10, 'L': 11, 'M': 12, 'N': 13, 'O': 14, 'P': 15, 'R': 16, 'S': 17, 'T': 18, 'U': 19, 'V': 20, 'Y': 21, 'Z': 22, 'del': 23, 'nothing': 24, 'space': 25}\n",
    "    for x,y in code.items():\n",
    "        if n==y:\n",
    "            return x\n",
    "# Function to Extract Feature from images or Frame\n",
    "def extract_featureTur(input_image):\n",
    "    mp_hands = mp.solutions.hands\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "    image = cv2.imread(input_image)\n",
    "\n",
    "    with mp_hands.Hands(static_image_mode=True, max_num_hands=2, min_detection_confidence=0.1) as hands:\n",
    "        while True:\n",
    "            results = hands.process(cv2.flip(cv2.cvtColor(image, cv2.COLOR_BGR2RGB), 1))\n",
    "            image_height, image_width, _ = image.shape\n",
    "            # Print handedness (left v.s. right hand).\n",
    "            # Caution : Uncomment these print command will resulting long log of mediapipe log\n",
    "            #print(f'Handedness of {input_image}:')\n",
    "            #print(results.multi_handedness)\n",
    "\n",
    "            # Draw hand landmarks of each hand.\n",
    "            # Caution : Uncomment these print command will resulting long log of mediapipe log\n",
    "            #print(f'Hand landmarks of {input_image}:')\n",
    "            if not results.multi_hand_landmarks:\n",
    "                # Here we will set whole landmarks into zero as no handpose detected\n",
    "                # in a picture wanted to extract.\n",
    "\n",
    "                # Wrist Hand\n",
    "                hand_data=np.zeros(10)\n",
    "\n",
    "                # Set image to Zero\n",
    "                annotated_image = 0\n",
    "\n",
    "                # Return Whole Landmark and Image\n",
    "                return (hand_data,\n",
    "                        annotated_image)\n",
    "\n",
    "            annotated_image = cv2.flip(image.copy(), 1)\n",
    "            hand_data = []\n",
    "            # Iterate over all hands detected in the image.\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "\n",
    "                # Extract the coordinates of all 21 hand landmarks.\n",
    "                landmark_coordinates = []\n",
    "                for landmark in mp_hands.HandLandmark:\n",
    "                    landmark_coordinates.append(hand_landmarks.landmark[landmark].x * image_width)\n",
    "                    landmark_coordinates.append(hand_landmarks.landmark[landmark].y * image_height)\n",
    "                    landmark_coordinates.append(hand_landmarks.landmark[landmark].z)\n",
    "                mp_drawing.draw_landmarks(annotated_image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "                # Add the coordinates of all 21 hand landmarks for the current hand to the new array.\n",
    "                hand_data.extend(landmark_coordinates)\n",
    "            #If less than two hands are detected, fill the rest of the array with zeros.\n",
    "            if len(results.multi_hand_landmarks) < 2:\n",
    "                num_missing_hands = 2 - len(results.multi_hand_landmarks)\n",
    "                num_missing_coordinates = num_missing_hands * 21 * 3  # 21 landmarks * 3 coordinates (x, y, z)\n",
    "                hand_data.extend([0] * num_missing_coordinates)\n",
    "\n",
    "            hand_data = np.array(hand_data)\n",
    "            return  (hand_data, annotated_image)\n",
    "def combine_videos(video_paths, output_path):\n",
    "    # Open the first video to get video properties\n",
    "    first_video = cv2.VideoCapture(video_paths[0])\n",
    "    frame_width = int(first_video.get(3))\n",
    "    frame_height = int(first_video.get(4))\n",
    "    fps = first_video.get(5)\n",
    "    first_video.release()\n",
    "\n",
    "    # Define the codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # You can choose the codec based on your needs\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    # Iterate through each video and append frames to the output video\n",
    "    for video_path in video_paths:\n",
    "        video = cv2.VideoCapture(video_path)\n",
    "        while True:\n",
    "            ret, frame = video.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            out.write(frame)\n",
    "\n",
    "        video.release()\n",
    "\n",
    "    # Release the output video\n",
    "    out.release()\n",
    "\n",
    "def video_to_base64(video_path):\n",
    "    try:\n",
    "        with open(video_path, 'rb') as video_file:\n",
    "            video_binary = video_file.read()\n",
    "\n",
    "        video_base64 = base64.b64encode(video_binary).decode('utf-8')\n",
    "\n",
    "        return video_base64\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Error encoding video: {str(e)}')\n",
    "        return None\n",
    "\n",
    "def searchinVids(param, lang_words):\n",
    "    path = 'C:/Users/DELL/Downloads/signApp/langs/greek/'+lang_words+'/'+param+'.mp4'\n",
    "    if os.path.exists(path):\n",
    "        result = path\n",
    "#     else:\n",
    "#         result = {'word':param,'vid':''}\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "@app.route('/text_to_sign', methods=['POST'])\n",
    "def fun4():\n",
    "    body = request.get_json()['message']['nameValuePairs']\n",
    "    try:\n",
    "        words= body['q']\n",
    "        cat = body['cat']\n",
    "    except Exception:\n",
    "        print(body)\n",
    "    words = words.split(' ')\n",
    "    vids = []\n",
    "    for word in words:\n",
    "        if 'english'in cat:\n",
    "            vids.append(searchinVids(word.upper(),cat))\n",
    "        else :\n",
    "            vids.append(searchinVids(word,cat))\n",
    "    combine_videos(vids,f'{app.config[\"UPLOAD_FOLDER\"]}vid.mp4')\n",
    "    result = video_to_base64(f'{app.config[\"UPLOAD_FOLDER\"]}vid.mp4')\n",
    "    return jsonify({'placement':str(result)})\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True, use_reloader=False,host=\"0.0.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf9b8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on all addresses.\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      " * Running on http://192.168.43.177:5000/ (Press CTRL+C to quit)\n",
      "192.168.43.1 - - [26/Dec/2023 01:39:42] \"POST /predictTrain HTTP/1.1\" 404 -\n",
      "192.168.43.177 - - [26/Dec/2023 01:40:02] \"GET / HTTP/1.1\" 404 -\n",
      "192.168.43.177 - - [26/Dec/2023 01:40:02] \"GET /favicon.ico HTTP/1.1\" 404 -\n",
      "192.168.43.1 - - [26/Dec/2023 01:40:37] \"POST /predictTrain HTTP/1.1\" 404 -\n"
     ]
    }
   ],
   "source": [
    "# from flask import Flask, request, jsonify\n",
    "# import base64\n",
    "# import os\n",
    "# import cv2\n",
    "# app = Flask(__name__)\n",
    "# UPLOAD_FOLDER = 'videos'\n",
    "# # Create the folder if it does not exist\n",
    "# os.makedirs(UPLOAD_FOLDER, exist_ok=True)\n",
    "# # Set the upload folder as an app configuration\n",
    "# app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\n",
    "# import cv2\n",
    "\n",
    "# def combine_videos(video_paths, output_path):\n",
    "#     # Open the first video to get video properties\n",
    "#     first_video = cv2.VideoCapture(video_paths[0])\n",
    "#     frame_width = int(first_video.get(3))\n",
    "#     frame_height = int(first_video.get(4))\n",
    "#     fps = first_video.get(5)\n",
    "#     first_video.release()\n",
    "\n",
    "#     # Define the codec and create VideoWriter object\n",
    "#     fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # You can choose the codec based on your needs\n",
    "#     out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "#     # Iterate through each video and append frames to the output video\n",
    "#     for video_path in video_paths:\n",
    "#         video = cv2.VideoCapture(video_path)\n",
    "#         while True:\n",
    "#             ret, frame = video.read()\n",
    "#             if not ret:\n",
    "#                 break\n",
    "#             out.write(frame)\n",
    "\n",
    "#         video.release()\n",
    "\n",
    "#     # Release the output video\n",
    "#     out.release()\n",
    "\n",
    "# def video_to_base64(video_path):\n",
    "#     try:\n",
    "#         with open(video_path, 'rb') as video_file:\n",
    "#             video_binary = video_file.read()\n",
    "\n",
    "#         video_base64 = base64.b64encode(video_binary).decode('utf-8')\n",
    "\n",
    "#         return video_base64\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f'Error encoding video: {str(e)}')\n",
    "#         return None\n",
    "\n",
    "# def searchinVids(param, lang_words):\n",
    "#     path = 'C:/Users/DELL/Downloads/signApp/langs/greek/'+lang_words+'/'+param+'.mp4'\n",
    "#     result = ''\n",
    "#     if os.path.exists(path):\n",
    "#         result = path\n",
    "# #     else:\n",
    "# #         result = {'word':param,'vid':''}\n",
    "\n",
    "#     return result\n",
    "\n",
    "\n",
    "# @app.route('/text_to_sign', methods=['POST'])\n",
    "# def fun4():\n",
    "#     body = request.get_json()['message']['nameValuePairs']\n",
    "#     try:\n",
    "#         words= body['q']\n",
    "#         cat = body['cat']\n",
    "#     except Exception:\n",
    "#         print(body)\n",
    "#     words = words.split(' ')\n",
    "#     vids = []\n",
    "#     for word in words:\n",
    "#         if 'english'in cat:\n",
    "#             if word is None:\n",
    "#                 continue\n",
    "#             else: \n",
    "#                 vids.append(searchinVids(word.upper()))\n",
    "#         else :\n",
    "#             vids.append(searchinVids(word,cat))\n",
    "#     combine_videos(vids,f'{app.config[\"UPLOAD_FOLDER\"]}vid.mp4')\n",
    "#     result = video_to_base64(f'{app.config[\"UPLOAD_FOLDER\"]}vid.mp4')\n",
    "#     return jsonify({'placement':str(result)})\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     app.run(debug=(True), use_reloader=False , host='0.0.0.0')\n",
    "\n",
    "# # app.config['UPLOAD_FOLDER']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff67ad06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tp` not found.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd13857",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
